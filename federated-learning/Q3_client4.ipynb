{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed50e0a",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9645f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.utils import shuffle \n",
    "import math\n",
    "import scipy\n",
    "import socket\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e687dc5",
   "metadata": {},
   "source": [
    "# Load, Shuffle and Normalize Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f368d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('mnist_client4.xlsx').values #4000 sample\n",
    "    \n",
    "# Shuffle data\n",
    "data = shuffle(data, random_state=42)\n",
    "\n",
    "num_data = data.shape[0]\n",
    "num_col = data.shape[1]\n",
    "num_class = len(np.unique(data[:,num_col-1]))\n",
    "\n",
    "# Normalize input data (shift 1 unit and normalize between 0,1)\n",
    "for ii in range(num_col-1): \n",
    "    data[:, ii] = ((data[:, ii] + 10) / 255) # max value for pixcels is 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2021a91",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_train = 0.7 # 70% Train 30% Test\n",
    "num_train = round(num_data * percent_train)\n",
    "num_test = num_data - num_train\n",
    "\n",
    "# Convert labels to one-hot encoding (necessary for multi-class classification) \n",
    "y_one_hot_train = np.zeros((num_train, num_class))\n",
    "y_one_hot_train[np.arange(num_train), data[:num_train, num_col-1].astype(int)] = 1\n",
    "\n",
    "y_one_hot_test = np.zeros((num_test, num_class))\n",
    "y_one_hot_test[np.arange(num_test), data[num_train:, num_col-1].astype(int)] = 1\n",
    "\n",
    "# Layers neurons\n",
    "# n0 = data.shape[1]-1\n",
    "# n1 = 35\n",
    "# n2 = 25\n",
    "# n3 = num_class  \n",
    "\n",
    "n0 = data.shape[1]-1\n",
    "n1 = 20\n",
    "n2 = 15\n",
    "n3 = num_class\n",
    "\n",
    "# learning_rate1\n",
    "learning_rate1 = 0.003\n",
    "learning_rate2 = 0.002\n",
    "learning_rate3 = 0.001 \n",
    "\n",
    "# local epoch\n",
    "epoch = 50   \n",
    "\n",
    "# Regularization parameters \n",
    "lambda_reg =0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b7c85",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_FUNC = 'leaky_relu'\n",
    "leaky_relu_alpha = 0.01\n",
    "def activation_function(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.maximum(0, x)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        return  1 /( 1 + (math.e)**(-1 * x))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        return 2/(1+ (math.e)**(-2*x))-1\n",
    "    elif(fun_name == 'leaky_relu'): \n",
    "        return np.where(x > 0, x, leaky_relu_alpha * x) \n",
    "\n",
    "def activation_function_derivative(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.where(x > 0, 1, 0)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        logsig_x = activation_function(x)\n",
    "        return logsig_x * (1 - logsig_x)\n",
    "    elif(fun_name == 'tansig'):\n",
    "        tansig_x = activation_function(x)\n",
    "        return 1 - tansig_x**2\n",
    "    elif(fun_name == 'leaky_relu'):\n",
    "        return np.where(x > 0, 1, leaky_relu_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b358b9",
   "metadata": {},
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the softmax output of a vector\n",
    "# epsilon to avoid divide by zero encountered\n",
    "epsilon = 0.000001\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    sum = exp_z.sum() \n",
    "    softmax_z = np.round(exp_z/(sum +epsilon ),3)\n",
    "    return softmax_z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09364d89",
   "metadata": {},
   "source": [
    "# Local Train Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece21dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(initials_parameters):\n",
    "    mse_train = np.zeros(epoch)  \n",
    "    w1 = initials_parameters[\"weights\"][0]\n",
    "    w2 = initials_parameters[\"weights\"][1]\n",
    "    w3 = initials_parameters[\"weights\"][2]\n",
    "    \n",
    "    b1 = initials_parameters[\"bias\"][0]\n",
    "    b2 = initials_parameters[\"bias\"][1]\n",
    "    b3 = initials_parameters[\"bias\"][2]\n",
    "    \n",
    "    # Initialize momentum parameters \n",
    "    beta = 0.9\n",
    "    vw1 = np.zeros((n1, n0))\n",
    "    vw2 = np.zeros((n2, n1))\n",
    "    vw3 = np.zeros((n3, n2)) \n",
    "\n",
    "    vb1 = np.zeros((n1,1))\n",
    "    vb2 = np.zeros((n2,1))\n",
    "    vb3 = np.zeros((n3,1)) \n",
    "\n",
    "    for t in range(epoch): \n",
    "        error_data_train = np.zeros(num_train)\n",
    "        output_data_train = np.zeros(num_train) \n",
    "\n",
    "        for i in range(num_train):\n",
    "            \n",
    "            # ******************************* feed-forward ******************************\n",
    "            input_data = data[i, :num_col-1].reshape(-1,1)\n",
    "            net1 = w1 @ input_data + b1\n",
    "            o1 = activation_function(net1)\n",
    "            net2 = w2 @ o1 + b2\n",
    "            o2 = activation_function(net2)\n",
    "            net3 = w3 @ o2 + b3  \n",
    "            o3 = net3\n",
    "            z = softmax(o3) \n",
    "            output_data_train[i] = np.argmax(z) \n",
    "\n",
    "            # ****************************** Backpropagation for a single sample ****************************** \n",
    "            # cross entropy error\n",
    "            output_layer_error = -(y_one_hot_train[i:i+1] @ np.log10(z,where=z>0)).reshape(-1,1).flatten() # where=z>0 to avoid RuntimeWarning: divide by zero encountered in log10\n",
    "\n",
    "            # update w3 with momentum\n",
    "            w3_old = w3\n",
    "            # dw3 = dE/dz*dz/do3*do3/dnet3*dnet3/dw3 = (z - target) * fprim_net3 * o2\n",
    "            dw3 =  (z - y_one_hot_train[i:i+1].reshape(-1,1)) @ o2.reshape(1,-1) \n",
    "            vw3 = beta * vw3 + (1 - beta) * dw3\n",
    "            w3 = w3 - learning_rate3 * vw3 - lambda_reg * w3\n",
    "\n",
    "            # update b3 with momentum \n",
    "            # db3 = dE/dz*dz/do3*do3/dnet3*dnet3/db3 = (z - target) * fprim_net3 * 1 \n",
    "            db3 = (z - y_one_hot_train[i:i+1].reshape(-1,1))\n",
    "            vb3 = beta * vb3 + (1 - beta) * db3 \n",
    "            b3 = b3 - learning_rate3 * vb3 - lambda_reg * b3\n",
    "\n",
    "            # update w2 with momentum\n",
    "            w2_old = w2\n",
    "            # dw2 = dE/z*dz/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/dw2 = (z - target) * fprim_net3 * w3 * fprim_net2 * o1\n",
    "            diag_fprim_net2 = np.diag(np.array(activation_function_derivative(net2)).flatten()) \n",
    "            dw2 = diag_fprim_net2 @ w3.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1)) @  o1.reshape(1,-1) \n",
    "            vw2 = beta * vw2 + (1 - beta) * dw2 \n",
    "            w2 = w2 - learning_rate2 * vw2 - lambda_reg * w2\n",
    "\n",
    "            # update b2 with momentum \n",
    "            # db2 =  dE/z*dz/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/db2 = (z - target) * fprim_net3 * w3 * fprim_net2 * 1\n",
    "            db2 = diag_fprim_net2 @ w3_old.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1))\n",
    "            vb2 = beta * vb2 + (1 - beta) * db2 \n",
    "            b2 = b2 - learning_rate2 * vb2 - lambda_reg * b2\n",
    "\n",
    "            # update w1 with momentum   \n",
    "            diag_fprim_net1 = np.diag(np.array(activation_function_derivative(net1)).flatten()) \n",
    "            # dw1 = dE/dz*dz/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/dw1 = (z - target) * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 * input_data\n",
    "            dw1 = diag_fprim_net1 @ w2_old.T @ diag_fprim_net2 @ w3_old.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1)) @ input_data.reshape(1,-1) \n",
    "            vw1 = beta * vw1 + (1 - beta) * dw1 \n",
    "            w1 = w1 - learning_rate1 * vw1 - lambda_reg * w1\n",
    "\n",
    "            # update b2 with momentum   \n",
    "            # db1 = dE/dz*dz/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/db1 = (z - target) * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 * 1\n",
    "            db1 = diag_fprim_net1 @ w2_old.T @ diag_fprim_net2 @ w3_old.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1)) \n",
    "            vb1 = beta * vb1 + (1 - beta) * db1\n",
    "            b1 = b1 - learning_rate1 * vb1 - lambda_reg * b1\n",
    "\n",
    "            error_data_train[i] = output_layer_error[0]\n",
    "\n",
    "        mse_train[t] = np.mean(error_data_train ** 2)\n",
    "        \n",
    "        # Plotting the training output\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.plot(data[:num_train, num_col-1], '-sr')\n",
    "        plt.plot(output_data_train, '-*b')\n",
    "        plt.xlabel('Train Data')\n",
    "        plt.ylabel('Output')   \n",
    "\n",
    "        # Plotting the training MSE\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.semilogy(np.arange(1, t + 1), mse_train[:t])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Train')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    \n",
    "        print('Epoch: {} \\t'.format(t+1))\n",
    "        print('MSE_train: ',mse_train[t])\n",
    "        print(\"\\n\\033[1;m\" + \"*\" * 125)\n",
    "\n",
    "    # Plotting the training Regression\n",
    "    plt.figure(2)\n",
    "    m_train , b_train = np.polyfit(data[:num_train, num_col - 1], output_data_train, 1)    \n",
    "    plt.scatter(data[:num_train, num_col - 1], output_data_train,facecolors='none',edgecolors='#104E8B')\n",
    "    plt.plot(data[:num_train, num_col - 1], m_train*data[:num_train, num_col - 1]+b_train,'r')\n",
    "    plt.title('Regression Train') \n",
    "    mse_train_result = mse_train[-1]  \n",
    "    \n",
    "    plot_confusion_matrix(output_data_train,\"Train Confusion Matrix\")\n",
    "    \n",
    "    # train_accuracy\n",
    "    train_accuracy = np.mean(output_data_train == data[:num_train,num_col-1] ) \n",
    "    print(f\"Accuracy on the train set: {train_accuracy * 100}%\")\n",
    "    \n",
    "    # return local updated parameters \n",
    "    return {\"client_id\":\"C4\" ,\"weights\":[w1,w2,w3],\"bias\":[b1,b2,b3],\"num_samples\":num_data, \"Train_MSE\":mse_train_result,'Train_Accuracy':train_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986a6e4",
   "metadata": {},
   "source": [
    "# Test Global Model(Aggregated Model) On Client Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_aggregated_model(aggregated_parameters):\n",
    "    w1 = aggregated_parameters[\"weights\"][0]\n",
    "    w2 = aggregated_parameters[\"weights\"][1]\n",
    "    w3 = aggregated_parameters[\"weights\"][2]\n",
    "    \n",
    "    b1 = aggregated_parameters[\"bias\"][0]\n",
    "    b2 = aggregated_parameters[\"bias\"][1]\n",
    "    b3 = aggregated_parameters[\"bias\"][2]\n",
    "    \n",
    "    current_server_round = aggregated_parameters[\"current_server_round\"]\n",
    "    \n",
    "    error_data_test = np.zeros(num_test)\n",
    "    output_data_test = np.zeros(num_test)\n",
    "    \n",
    "    for i in range(num_test):\n",
    "        input_data = data[num_train + i, :num_col-1].reshape(-1,1) \n",
    "        net1 = w1 @ input_data + b1 \n",
    "        o1 = activation_function(net1)\n",
    "        net2 = w2 @ o1 + b2\n",
    "        o2 = activation_function(net2)\n",
    "        net3 = w3 @ o2 + b3 \n",
    "        o3 = net3\n",
    "        z = softmax(o3) \n",
    "        output_data_test[i] = np.argmax(z)\n",
    "        error = -(y_one_hot_test[i:i+1] @ np.log10(z,where=z>0)).reshape(-1,1).flatten() \n",
    "        error_data_test[i] = error[0]\n",
    "        \n",
    "    mse_test[current_server_round] = np.mean(error_data_test ** 2)\n",
    "    \n",
    "    # Plotting the test output\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(2, 2, 1)    \n",
    "    plt.plot(data[num_train:, num_col-1], '-sr')\n",
    "    plt.plot(output_data_test, '-*b')\n",
    "    plt.xlabel('Test Data')\n",
    "    plt.ylabel('Output')\n",
    "    \n",
    "    # Plotting the test MSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.semilogy(np.arange(1, current_server_round + 1), mse_test[:current_server_round])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Test')\n",
    "\n",
    "    # Plotting Regression Test\n",
    "    plt.figure()\n",
    "    m_test , b_test = np.polyfit(data[num_train:, num_col - 1], output_data_test, 1)  \n",
    "    plt.scatter(data[num_train:, num_col - 1], output_data_test,facecolors='none',edgecolors='#104E8B')\n",
    "    plt.plot(data[num_train:, num_col - 1], m_test*data[num_train:,num_col - 1]+b_test,'r')\n",
    "    plt.title('Regression Test')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plot_confusion_matrix(output_data_test,\"Test Confusion Matrix\")\n",
    "    \n",
    "    print('current_server_round: {} \\t'.format(current_server_round+1))\n",
    "    print('MSE_Test: ' ,mse_test[current_server_round])\n",
    "    \n",
    "    \n",
    "    test_accuracy = np.mean(output_data_test == data[num_train:,num_col-1]) \n",
    "    print(f\"Accuracy on the test set: {test_accuracy * 100}%\") \n",
    "    \n",
    "    return {\"client_id\":\"C4\" ,\"Test_MSE\": mse_test[current_server_round],'Test_Accuracy':test_accuracy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb2126",
   "metadata": {},
   "source": [
    "# Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predicted_classes,title):\n",
    "   # Assuming you have converted regression predictions to classes \n",
    "    actual_classes = data[:num_train,num_col-1]  \n",
    "    \n",
    "    # Create a confusion matrix-like matrix\n",
    "    confusion_matrix = np.zeros((num_class, num_class))\n",
    "\n",
    "    # Fill the confusion matrix\n",
    "    for actual, predicted in zip(actual_classes, predicted_classes):\n",
    "        confusion_matrix[actual.astype(int)][predicted.astype(int)] += 1 \n",
    "\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure()\n",
    "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Annotate the plot with numbers\n",
    "    for i in range(num_class):\n",
    "        for j in range(num_class):\n",
    "            plt.text(j, i, str(int(confusion_matrix[i, j])), fontsize=12, ha='center', va='center')  # Corrected\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(np.arange(num_class), np.arange(0, num_class))\n",
    "    plt.yticks(np.arange(num_class), np.arange(0, num_class))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0e5ed",
   "metadata": {},
   "source": [
    "# Client-Server Communication Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2349f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Receive data until fully received (For Huge data => chunk by chunk)\n",
    "def receive_all(socket, length):\n",
    "    data = b''\n",
    "    while len(data) < length:\n",
    "        packet = socket.recv(length - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data\n",
    "\n",
    "# Send data in chunks(For Huge data => chunk by chunk)\n",
    "def send_all(socket, data):\n",
    "    data_pickle = pickle.dumps(data)\n",
    "    data_size = len(data_pickle)\n",
    "    socket.sendall(data_size.to_bytes(4, 'big'))  # Send data size first\n",
    "\n",
    "    sent = 0\n",
    "    while sent < data_size:\n",
    "        chunk = data_pickle[sent:sent+4096]  # Send in chunks\n",
    "        socket.sendall(chunk)\n",
    "        sent += len(chunk)\n",
    "\n",
    "# Initialize client socket\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_address = ('localhost', 1111)\n",
    "client_socket.connect(server_address)\n",
    "print('Client socket initialized')\n",
    "\n",
    "# Receive initial weights from server\n",
    "initial_parameters_size = int.from_bytes(client_socket.recv(4), 'big')  # Receive data size first\n",
    "initial_parameters_data = receive_all(client_socket, initial_parameters_size)\n",
    "initial_parameters = pickle.loads(initial_parameters_data)\n",
    "print('Received initial weights from server')\n",
    "\n",
    "\n",
    "# initialize mse_test according to server_rounds (after each aggregation proccess evaluate global-model with local test-data)\n",
    "server_rounds = initial_parameters[\"server_rounds\"] \n",
    "mse_test = np.zeros(server_rounds)\n",
    "\n",
    "while True: \n",
    "    # get server signal => ready: start new learning process on client local data |  terminate: federated-learning process terminate by server\n",
    "    signal = pickle.loads(client_socket.recv(4096))\n",
    "    print('Server Signal: ', signal)\n",
    "    \n",
    "    if 'terminate' in signal:\n",
    "        print(\"Received termination signal. Terminating client.\")\n",
    "        break \n",
    "\n",
    "    print('Start Local Training')\n",
    "\n",
    "    # start train model with initial parameters already recieved from server on local Data\n",
    "    data_to_send = train_local_model(initial_parameters)\n",
    "    data_to_send['num_samples'] = num_data\n",
    "    \n",
    "    print('End Local Training')\n",
    "    \n",
    "    # send trained parameters back to server to aggregate\n",
    "    send_all(client_socket, data_to_send)\n",
    "\n",
    "    print('Sent Updated Params To Server')\n",
    "    \n",
    "    # receive aggregated model from server\n",
    "    aggregated_parameters_size = int.from_bytes(client_socket.recv(4), 'big')  # Receive data size first\n",
    "    aggregated_parameters_data = receive_all(client_socket, aggregated_parameters_size)\n",
    "    aggregated_parameters = pickle.loads(aggregated_parameters_data)\n",
    "    \n",
    "    print('Received Aggregated Model From Server')\n",
    "    \n",
    "    print('Start Evaluating Aggregated Model With Local Test Data') \n",
    "        \n",
    "    # evaluate aggregated model with test-data\n",
    "    test_result = evaluate_aggregated_model(aggregated_parameters)\n",
    "    \n",
    "    print('End Evaluating Aggregated Model With Local Test Data')\n",
    "    \n",
    "    send_all(client_socket, test_result)\n",
    "\n",
    "    print('Sent Evaluated Aggregated Model With Local Test Data Results To Server')\n",
    "    \n",
    "    # set updated-params to inital-params of next federated-learning local training process on client  \n",
    "    initial_parameters = aggregated_parameters\n",
    "\n",
    "    print(\"\\n\\033[1;m\" + \"*\" * 125)   \n",
    "    \n",
    "# Close connection\n",
    "client_socket.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Excel file\n",
    "data = pd.read_excel('Lorenz Dataset.xlsx', header=None).values  \n",
    "# data = pd.read_excel('Tehran Stock Exchange.xlsx', header=None).values\n",
    "# data = pd.read_excel('ECG Datasets.xlsx', header=None).values\n",
    "\n",
    "data_size = len(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data\n",
    "for ii in range(len(data[0])): \n",
    "    data[:, ii] = data[:, ii] / np.max(data[:, ii])  \n",
    "\n",
    "num_feature = len(data[0]) - 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for row in data:\n",
    "#     for elem in row:\n",
    "#         if abs(elem) > 1:\n",
    "#             print(row, elem) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lorenz Dataset\n",
    "np.random.seed(42)\n",
    "epochs = 100 \n",
    "regularization_strength = 0.005\n",
    "learning_rate = 0.0001 \n",
    "\n",
    "#Stock Dataset\n",
    "# np.random.seed(41)\n",
    "# epochs = 100 \n",
    "# regularization_strength = 0.008\n",
    "# learning_rate = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMDHNeuron:\n",
    "    def __init__(self, x1_index, x2_index):\n",
    "        self.x1_index = x1_index\n",
    "        self.x2_index = x2_index\n",
    "        self.weights = np.random.uniform(-1, 1, 6) \n",
    "        self.train_mse_history = []\n",
    "        self.validation_mse = None\n",
    "        \n",
    "    def train(self, x1_train, x2_train, actual_output, epochs,learning_rate, regularization_strength):\n",
    "        self.train_mse_history = []\n",
    "        for epoch in range(epochs):\n",
    "            for i in range(len(x1_train)):\n",
    "                predicted_output = self.predict(x1_train[i], x2_train[i])\n",
    "                error = predicted_output - actual_output[i]\n",
    "#                 print('index: ', i)\n",
    "#                 print('error: ', error)\n",
    "#                 print('x1_train[i]**2: ',x1_train[i]**2)\n",
    "#                 print('x2_train[i]**2: ',x2_train[i]**2)\n",
    "                \n",
    "                gradient = np.array([\n",
    "                    -1 * error * x1_train[i]**2,\n",
    "                    -1 * error * x2_train[i]**2,\n",
    "                    -1 * error * x1_train[i] * x2_train[i],\n",
    "                    -1 * error * x1_train[i],\n",
    "                    -1 * error * x2_train[i],\n",
    "                    -1 * error\n",
    "                ])\n",
    "                \n",
    "                # L2 regularization\n",
    "                regularization_term = 2 * regularization_strength * self.weights  \n",
    "                \n",
    "                self.weights = self.weights - learning_rate * gradient - regularization_term\n",
    "            \n",
    "            train_mse = self.calculate_mse(x1_train, x2_train, actual_output)\n",
    "#             print(train_mse)\n",
    "            self.train_mse_history.append(train_mse)\n",
    "        \n",
    "        # calculate validation MSE\n",
    "        self.validation_mse = self.calculate_mse(validation_data[:,self.x1_index],validation_data[:,self.x2_index],validation_output)\n",
    "        \n",
    "    def predict(self, x1, x2):\n",
    "        return self.weights[0] * x1**2 + self.weights[1] * x2**2 + self.weights[2] * x1 * x2 + self.weights[3] * x1  + self.weights[4] * x2 + self.weights[5]\n",
    "\n",
    "    def calculate_mse(self, x1, x2, target):\n",
    "        predicted_output = self.predict(x1, x2)\n",
    "        mse = np.mean((predicted_output - target) ** 2)\n",
    "        return mse\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training, validation, and testing sets\n",
    "train_size = int(0.7 * data_size)\n",
    "validation_size = int(0.15 * data_size)\n",
    "\n",
    "train_data, validation_data, test_data = data[:train_size,:num_feature], data[train_size:train_size + validation_size,:num_feature], data[train_size + validation_size:,:num_feature]\n",
    "train_output, validation_output, test_output = data[:train_size,num_feature], data[train_size:train_size + validation_size,num_feature], data[train_size + validation_size:,num_feature]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_gmdh_layer_neurons = []\n",
    "for i in range(num_feature):\n",
    "    for j in range(i + 1, num_feature):\n",
    "        neuron = GMDHNeuron(i,j)\n",
    "        neuron.train(train_data[:, i], train_data[:, j], train_output, epochs,learning_rate, regularization_strength)\n",
    "        first_gmdh_layer_neurons.append(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in first_gmdh_layer_neurons:\n",
    "    # Plotting the training data and output\n",
    "    plt.figure(figsize=(20, 8)) \n",
    "    plt.semilogy(np.arange(1, epochs + 1 ), neuron.train_mse_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Train') \n",
    "    plt.show()\n",
    "    print('selected_neurons-indexs: ', neuron.x1_index,neuron.x2_index)\n",
    "    print(\"Final Weights:\", neuron.weights)\n",
    "    print(\"Final Train MSE:\", neuron.train_mse_history[-1])\n",
    "    print(\"Final Validation MSE:\", neuron.validation_mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_gmdh_layer_train_output = [neuron.predict(train_data[:, neuron.x1_index], train_data[:, neuron.x2_index]) for neuron in first_gmdh_layer_neurons]\n",
    "train_data = np.array(list(zip(*first_gmdh_layer_train_output)))\n",
    "\n",
    "first_gmdh_layer_validation_output = [neuron.predict(validation_data[:, neuron.x1_index], validation_data[:, neuron.x2_index]) for neuron in first_gmdh_layer_neurons]\n",
    "validation_data = np.array(list(zip(*first_gmdh_layer_validation_output))) \n",
    "\n",
    "first_gmdh_layer_test_output = [neuron.predict(test_data[:, neuron.x1_index], test_data[:, neuron.x2_index]) for neuron in first_gmdh_layer_neurons]\n",
    "test_data  = np.array(list(zip(*first_gmdh_layer_test_output))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the train_data\n",
    "for ii in range(len(train_data[0])): \n",
    "    train_data[:, ii] = train_data[:, ii] / np.max(train_data[:, ii])\n",
    "    \n",
    "# Normalize the validation_data\n",
    "for ii in range(len(validation_data[0])): \n",
    "    validation_data[:, ii] = validation_data[:, ii] / np.max(validation_data[:, ii])\n",
    "\n",
    "# Normalize the test_data\n",
    "for ii in range(len(test_data[0])): \n",
    "    test_data[:, ii] = test_data[:, ii] / np.max(test_data[:, ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lorenz Dataset\n",
    "np.random.seed(42)\n",
    "epochs = 100 \n",
    "regularization_strength = 0.005\n",
    "learning_rate = 0.0015\n",
    "\n",
    "#Stock Dataset\n",
    "# np.random.seed(41)\n",
    "# epochs = 100 \n",
    "# regularization_strength = 0.005\n",
    "# learning_rate = 0.00015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_gmdh_layer_neurons = []\n",
    "for i in range(len(train_data[0])):\n",
    "    for j in range(i + 1, len(train_data[0])):\n",
    "        neuron = GMDHNeuron(i,j)\n",
    "        neuron.train(train_data[:, i], train_data[:, j], train_output, epochs,learning_rate, regularization_strength)\n",
    "        second_gmdh_layer_neurons.append(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for neuron in second_gmdh_layer_neurons:\n",
    "    # Plotting the training data and output\n",
    "    plt.figure(figsize=(20, 8)) \n",
    "    plt.semilogy(np.arange(1, epochs + 1 ), neuron.train_mse_history)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Train') \n",
    "    plt.show()\n",
    "    print('selected_neurons-indexs: ', neuron.x1_index,neuron.x2_index)\n",
    "    print(\"Final Weights:\", neuron.weights)\n",
    "    print(\"Final Train MSE:\", neuron.train_mse_history[-1])\n",
    "    print(\"Final Validation MSE:\", neuron.validation_mse)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_gmdh_layer_neurons = sorted(second_gmdh_layer_neurons, key=lambda x: x.validation_mse)\n",
    "# Select the top two neurons with the lowest validation_mse\n",
    "second_gmdh_layer_neurons = second_gmdh_layer_neurons[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_gmdh_layer_train_output = [neuron.predict(train_data[:, neuron.x1_index], train_data[:, neuron.x2_index]) for neuron in second_gmdh_layer_neurons]\n",
    "train_data = np.array(list(zip(*second_gmdh_layer_train_output)))\n",
    "\n",
    "second_gmdh_layer_validation_output = [neuron.predict(validation_data[:, neuron.x1_index], validation_data[:, neuron.x2_index]) for neuron in second_gmdh_layer_neurons]\n",
    "validation_data = np.array(list(zip(*second_gmdh_layer_validation_output))) \n",
    "\n",
    "second_gmdh_layer_test_output = [neuron.predict(test_data[:, neuron.x1_index], test_data[:, neuron.x2_index]) for neuron in second_gmdh_layer_neurons]\n",
    "test_data  = np.array(list(zip(*second_gmdh_layer_test_output))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the input data\n",
    "for ii in range(len(train_data[0])): \n",
    "    train_data[:, ii] = train_data[:, ii] / np.max(train_data[:, ii])\n",
    "    \n",
    "# Normalize the input data\n",
    "for ii in range(len(validation_data[0])): \n",
    "    validation_data[:, ii] = validation_data[:, ii] / np.max(validation_data[:, ii])\n",
    "\n",
    "# Normalize the input data\n",
    "for ii in range(len(test_data[0])): \n",
    "    test_data[:, ii] = test_data[:, ii] / np.max(test_data[:, ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_FUNC = 'leaky_relu'\n",
    "leaky_relu_alpha = 0.01\n",
    "def activation_function(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.maximum(0, x)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        return  1 /( 1 + (math.e)**(-1 * x))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        return 2/(1+ (math.e)**(-2*x))-1\n",
    "    elif(fun_name == 'leaky_relu'): \n",
    "        return np.where(x > 0, x, leaky_relu_alpha * x) \n",
    "\n",
    "def activation_function_derivative(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.where(x > 0, 1, 0)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        a = activation_function(x)\n",
    "        a = np.reshape(a, (-1,1))\n",
    "        b = 1 - activation_function(x)\n",
    "        b = np.reshape(b, (-1,1))\n",
    "        b = np.transpose(b)\n",
    "        return np.diag(np.diag(np.matmul(a,b)))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        tansig_x = activation_function(x)\n",
    "        return 1 - tansig_x**2\n",
    "    elif(fun_name == 'leaky_relu'):\n",
    "        return np.where(x > 0, 1, leaky_relu_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lorenz parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of input, hidden, and output neurons\n",
    "input_neurons = train_data.shape[1]\n",
    "l1_neurons = 15\n",
    "output_neurons = 1  # Linear activation for regression \n",
    "\n",
    "# Initialize the weights with random values in range (-1,1)\n",
    "np.random.seed(1)\n",
    "w1 = 2 * np.random.random((input_neurons, l1_neurons)) - 1\n",
    "w2_L = 2 * np.random.random((l1_neurons, output_neurons)) - 1\n",
    "w2_U = 2 * np.random.random((l1_neurons, output_neurons)) - 1\n",
    "\n",
    "\n",
    "# Initialize the biases with random values in range (-1,1) \n",
    "b1 = 2 * np.random.random(l1_neurons) - 1\n",
    "b2_L = 2 * np.random.random(output_neurons) - 1\n",
    "b2_U = 2 * np.random.random(output_neurons) - 1\n",
    "\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.005\n",
    "epochs = 400  # Train sample by sample \n",
    "\n",
    "mse_train = np.zeros(epochs)\n",
    "mse_test = np.zeros(epochs)\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.5\n",
    "\n",
    "lambda_reg = 0.00001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the number of input, hidden, and output neurons\n",
    "# input_neurons = train_data.shape[1]\n",
    "# l1_neurons = 10\n",
    "# output_neurons = 1  # Linear activation for regression \n",
    "\n",
    "# # Initialize the weights with random values in range (-1,1)\n",
    "# np.random.seed(1)\n",
    "# w1 = 2 * np.random.random((input_neurons, l1_neurons)) - 1\n",
    "# w2_L = 2 * np.random.random((l1_neurons, output_neurons)) - 1\n",
    "# w2_U = 2 * np.random.random((l1_neurons, output_neurons)) - 1\n",
    "\n",
    "\n",
    "# # Initialize the biases with random values in range (-1,1) \n",
    "# b1 = 2 * np.random.random(l1_neurons) - 1\n",
    "# b2_L = 2 * np.random.random(output_neurons) - 1\n",
    "# b2_U = 2 * np.random.random(output_neurons) - 1\n",
    "\n",
    "\n",
    "# # Training parameters\n",
    "# learning_rate = 0.003\n",
    "# epochs = 400  # Train sample by sample \n",
    "\n",
    "# mse_train = np.zeros(epochs)\n",
    "# mse_test = np.zeros(epochs)\n",
    "\n",
    "# alpha = 0.5\n",
    "# beta = 0.5\n",
    "\n",
    "# lambda_reg = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the MLP for regression\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    error_data_train = np.zeros(len(train_data))\n",
    "    output_data_train = np.zeros(len(train_data))\n",
    "    for i in range(len(train_data)):\n",
    "        \n",
    "        #-------------------------------- Feed Forward -------------------------------------\n",
    "        input_layer = train_data[i:i+1] \n",
    "        \n",
    "        net1 = np.dot(input_layer, w1) + b1  # net1 = x * w1 + b1\n",
    "        o1   = activation_function(net1) #  o1 = f(net1)\n",
    "        \n",
    "        net2_L = np.dot(o1, w2_L) + b2_L # net2_L = o1 * w2 + b2_L\n",
    "        net2_U = np.dot(o1, w2_U) + b2_U # net2_U = o1 * w2 + b2_U\n",
    "        \n",
    "        f2_L = activation_function(net2_L)\n",
    "        f2_U = activation_function(net2_U)\n",
    "        \n",
    "        o2_L = min(f2_L,f2_U)\n",
    "        o2_U = max(f2_L,f2_U)\n",
    "        \n",
    "        o2 = alpha * o2_U + beta * o2_L\n",
    "        output_data_train[i] = o2      \n",
    "        #-------------------------------- Backpropagation ----------------------------------- \n",
    "        output_layer_error = train_output[i:i+1] - o2\n",
    "        if(f2_L <= f2_U): # => o2_L = f2_L(net2_L) = f2_L(w2_L.T * o1) , o2_U = f2_U(net2_U) = f2_U(w2_U.T * o1)\n",
    "            \n",
    "            # update w2_L\n",
    "            # dE/dw2_L = dE/de * de/do2 * do2/do2_L * do2_L/dnet2_L * dnet2_L/dw2_L = e * -1 * beta * fprim_net2_L *  o1 \n",
    "            w2_L_old = w2_L\n",
    "            w2_L = w2_L +  learning_rate * beta * o1.T @ output_layer_error  - lambda_reg * w2_L \n",
    "            \n",
    "            # update w2_U\n",
    "            # dE/dw2_U = dE/de * de/do2 * do2/do2_U * do2_U/dnet2_U * dnet2_U/dw2_U = e * -1 * alpha * fprim_net2_U *  o1\n",
    "            w2_U_old = w2_U\n",
    "            w2_U = w2_U +  learning_rate * alpha * o1.T @ output_layer_error  - lambda_reg * w2_U\n",
    "            \n",
    "            # update b2_L\n",
    "            # dE/db2_L = dE/de * de/do2 * do2/do2_L * do2_L/dnet2_L * dnet2_L/db2_L = e * -1 * beta * fprim_net2_L * 1\n",
    "            b2_L = b2_L +  learning_rate * beta * output_layer_error  - lambda_reg * b2_L\n",
    "        \n",
    "            # update b2_U\n",
    "            # dE/db2_U = dE/de * de/do2 * do2/do2_U * do2_U/dnet2_U * dnet2_U/db2_U = e * -1 * alpha * fprim_net2_U * 1\n",
    "            b2_U = b2_U +  learning_rate * alpha * output_layer_error  - lambda_reg * b2_U\n",
    "            \n",
    "           \n",
    "            # update w1\n",
    "            # dE/dw1 = dE/de * de/do2 * (do2/do2_L * do2_L/dnet2_L * dnet2_L/do1 + do2/do2_U * do2_U/dnet2_U * dnet2_U/do1) * do1/dnet1 * dnet1/dw1\n",
    "            # = e * -1 * (beta * fprim_net2_L * w2_L + alpha * fprim_net2_U * w2_U) * fprim_net1 * input_layer\n",
    "            \n",
    "            # fprim_net2_L = 1 , fprim_net2_U = 1\n",
    "            # Create a diagonal matrix\n",
    "            fprim_net1 = np.array(activation_function_derivative(net1))[0] \n",
    "            diag_matrix_fprim_net1 = np.diag(fprim_net1)\n",
    "            w1 = w1 + learning_rate * input_layer.T @ output_layer_error * (beta * w2_L_old  + alpha * w2_U_old).T @ diag_matrix_fprim_net1  - lambda_reg * w1\n",
    "\n",
    "            \n",
    "            # update b1\n",
    "            # dE/db1 = dE/de * de/do2 * (do2/do2_L * do2_L/dnet2_L * dnet2_L/do1 + do2/do2_U * do2_U/dnet2_U * dnet2_U/do1) * do1/dnet1 * dnet1/db1\n",
    "            # = e * -1 * (beta * fprim_net2_L * w2_L + alpha * fprim_net2_U * w2_U) * fprim_net1 * 1\n",
    "            b1 = b1 + learning_rate * output_layer_error @ (beta * w2_L_old  + alpha * w2_U_old).T @ diag_matrix_fprim_net1  - lambda_reg * b1\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        elif(f2_L > f2_U): # => o2_L = f2_U(net2_U) = f2_U(w2_U.T * o1) , o2_U = f2_L(net2_L) = f2_L(w2_L.T * o1)\n",
    "            \n",
    "            # update w2_L\n",
    "            # dE/dw2_L = dE/de * de/do2 * do2/do2_U * do2_U/dnet2_L * dnet2_L/dw2_L = e * -1 * alpha * fprim_net2_L *  o1 \n",
    "            w2_L_old = w2_L\n",
    "            w2_L = w2_L +  learning_rate * alpha * o1.T @ output_layer_error  - lambda_reg * w2_L\n",
    "            \n",
    "            # update w2_U\n",
    "            # dE/dw2_U = dE/de * de/do2 * do2/do2_L * do2_L/dnet2_U * dnet2_U/dw2_U = e * -1 * beta * fprim_net2_U *  o1\n",
    "            w2_U_old = w2_U\n",
    "            w2_U = w2_U +  learning_rate * beta * o1.T @ output_layer_error  - lambda_reg * w2_U\n",
    "            \n",
    "            # update b2_L\n",
    "            # dE/db2_L = dE/de * de/do2 * do2/do2_U * do2_U/dnet2_L * dnet2_L/db2_L = e * -1 * alpha * fprim_net2_L * 1\n",
    "            b2_L = b2_L +  learning_rate * alpha * output_layer_error  - lambda_reg * b2_L\n",
    "        \n",
    "            # update b2_U\n",
    "            # dE/db2_U = dE/de * de/do2 * do2/do2_L * do2_L/dnet2_U * dnet2_U/db2_U = e * -1 * beta * fprim_net2_U * 1\n",
    "            b2_U = b2_U +  learning_rate * beta * output_layer_error - lambda_reg * b2_U\n",
    "            \n",
    "           \n",
    "            # update w1\n",
    "            # dE/dw1 = dE/de * de/do2 * (do2/do2_U * do2_U/dnet2_L * dnet2_L/do1 + do2/do2_L * do2_L/dnet2_U * dnet2_U/do1) * do1/dnet1 * dnet1/dw1\n",
    "            # = e * -1 * (alpha * fprim_net2_L * w2_L + beta * fprim_net2_U * w2_U) * fprim_net1 * input_layer\n",
    "            \n",
    "            # fprim_net2_L = 1 , fprim_net2_U = 1\n",
    "            # Create a diagonal matrix\n",
    "            fprim_net1 = np.array(activation_function_derivative(net1))[0] \n",
    "            diag_matrix_fprim_net1 = np.diag(fprim_net1)\n",
    "#             print('input_layer: ',input_layer.shape)\n",
    "#             print('w2_L_old: ', w2_L_old.shape)\n",
    "#             print('w2_U_old: ', w2_U_old.shape)\n",
    "#             print('diag_matrix_fprim_net1: ', diag_matrix_fprim_net1.shape)\n",
    "#             print('w1: ', w1.shape)\n",
    "#             print(input_layer.T.shape,output_layer_error.shape,(w2_L_old + w2_U_old).T.shape,diag_matrix_fprim_net1.shape)\n",
    "#             dw1 = -1 * input_layer.T * output_layer_error * (alpha * w2_L_old  + beta * w2_U_old).T @ diag_matrix_fprim_net1\n",
    "            w1 = w1 + learning_rate * input_layer.T @ output_layer_error * (alpha * w2_L_old  + beta * w2_U_old).T @ diag_matrix_fprim_net1  - lambda_reg * w1\n",
    "\n",
    "            \n",
    "            # update b1\n",
    "            # dE/db1 = dE/de * de/do2 * (do2/do2_U * do2_U/dnet2_L * dnet2_L/do1 + do2/do2_L * do2_L/dnet2_U * dnet2_U/do1) * do1/dnet1 * dnet1/db1\n",
    "            # = e * -1 * (beta * fprim_net2_L * w2_L + alpha * fprim_net2_U * w2_U) * fprim_net1 * 1\n",
    "            b1 = b1 + learning_rate * output_layer_error @ (alpha * w2_L_old  + beta * w2_U_old).T @ diag_matrix_fprim_net1  - lambda_reg * b1\n",
    "\n",
    "              \n",
    "        error_data_train[i] = output_layer_error\n",
    "    \n",
    "    mse_train[epoch] = np.mean(error_data_train ** 2)\n",
    "    \n",
    "    # Testing the trained MLP for regression \n",
    "    error_data_test = np.zeros(len(test_data))\n",
    "    output_data_test = np.zeros(len(test_data))\n",
    "    for i in range(len(test_data)):\n",
    "        input_layer = test_data[i:i+1] \n",
    "        \n",
    "        net1 = np.dot(input_layer, w1) + b1  # net1 = x * w1 + b1\n",
    "        o1   = activation_function(net1) #  o1 = f(net1)\n",
    "        \n",
    "        net2_L = np.dot(o1, w2_L) + b2_L # net2_L = o1 * w2 + b2_L\n",
    "        net2_U = np.dot(o1, w2_U) + b2_U # net2_U = o1 * w2 + b2_U\n",
    "        \n",
    "        f2_L = activation_function(net2_L)\n",
    "        f2_U = activation_function(net2_U)\n",
    "        \n",
    "        o2_L = min(f2_L,f2_U)\n",
    "        o2_U = max(f2_L,f2_U)\n",
    "        \n",
    "        o2 = alpha * o2_U + beta * o2_L\n",
    "#         print(o2)\n",
    "        output_data_test[i] = o2\n",
    "        error_data_test[i] = test_output[i] - o2\n",
    "    \n",
    "    mse_test[epoch] = np.mean(error_data_test ** 2)\n",
    "        \n",
    "    # Plotting the training data and output\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(train_output)\n",
    "    plt.plot(output_data_train, 'r', linewidth=0.5)\n",
    "    plt.xlabel('Train Data')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "    # Plotting the training MSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.semilogy(np.arange(1, epoch + 1), mse_train[:epoch])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Train') \n",
    "    \n",
    "    print('Epoch: {} \\t'.format(epoch+1)) \n",
    "    print('MSE_train: {:.4f}'.format(mse_train[epoch]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "      \n",
    "    print(\"\\n\\033[1;m\" + \"*\" * 125)\n",
    "         \n",
    "        \n",
    "print(\"****************************** Training completed *******************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training data and output\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(train_output)\n",
    "plt.plot(output_data_train, 'r', linewidth=0.5)\n",
    "plt.xlabel('Train Data')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "# Plotting the training MSE \n",
    "plt.subplot(2, 2, 2)\n",
    "plt.semilogy(np.arange(1, epoch + 1), mse_train[:epoch])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Train') \n",
    "\n",
    "\n",
    "# Plotting the test data and output\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(test_output)\n",
    "plt.plot(output_data_test+0.1, 'r', linewidth=0.5)\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "# Plotting the test MSE\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.semilogy(np.arange(1, epoch + 1), mse_test[:epoch])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Test')  \n",
    "\n",
    "\n",
    "print('MSE_train: ',mse_train[epoch])\n",
    "print('MSE_test: ',mse_test[epoch]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "m_train , b_train = np.polyfit(train_output, output_data_train, 1)    \n",
    "plt.scatter(train_output, output_data_train,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(train_output, m_train*train_output+b_train,'r') \n",
    "plt.title('Regression Train') \n",
    "\n",
    "plt.figure(3)\n",
    "m_test , b_test = np.polyfit(test_output, output_data_test, 1)  \n",
    "plt.scatter(test_output, output_data_test,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(test_output, m_test*test_output+b_test,'r')\n",
    "plt.title('Regression Test')\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse_train_result = mse_train[-1]\n",
    "mse_test_result = mse_test[-1]\n",
    "\n",
    "print(\"Final MSE on Train Data:\", mse_train_result)\n",
    "print(\"Final MSE on Test Data:\", mse_test_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP4cHBUTTRrxiRZtggpjKCM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

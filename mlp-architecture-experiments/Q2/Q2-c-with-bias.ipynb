{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Split to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Excel file\n",
    "# data = pd.read_excel('Temperature Dataset.xlsx', header=None).values\n",
    "# data = pd.read_excel('ECG Datasets.xlsx', header=None).values\n",
    "data = pd.read_excel('Lorenz Dataset.xlsx', header=None).values\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "# Normalize the input data\n",
    "for ii in range(4): \n",
    "    data[:, ii] = data[:, ii] / np.max(data[:, ii]) \n",
    "    \n",
    "# Split the dataset into a training and testing set\n",
    "# X_train, X_test,y_train, y_test = train_test_split(data[:num_data, :3], data[:num_data,3], test_size=0.25, random_state=42)\n",
    "\n",
    "split_ratio_train = 0.7\n",
    "\n",
    "split_line_number = int(np.shape(data)[0] * split_ratio_train)\n",
    "X_train = data[:split_line_number, :3]\n",
    "y_train = data[:split_line_number, 3]\n",
    "\n",
    "other_data = data[split_line_number:, :4]\n",
    "\n",
    "X_test = data[split_line_number:len(data), :3]\n",
    "y_test = data[split_line_number:len(data), 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_FUNC = 'leaky_relu'\n",
    "alpha = 0.01\n",
    "def activation_function(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.maximum(0, x)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        return  1 /( 1 + (math.e)**(-1 * x))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        return 2/(1+ (math.e)**(-2*x))-1\n",
    "    elif(fun_name == 'leaky_relu'): \n",
    "        return np.where(x > 0, x, alpha * x) \n",
    "\n",
    "def activation_function_derivative(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.where(x > 0, 1, 0)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        a = activation_function(x)\n",
    "        a = np.reshape(a, (-1,1))\n",
    "        b = 1 - activation_function(x)\n",
    "        b = np.reshape(b, (-1,1))\n",
    "        b = np.transpose(b)\n",
    "        return np.diag(np.diag(np.matmul(a,b)))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        tansig_x = activation_function(x)\n",
    "        return 1 - tansig_x**2\n",
    "    elif(fun_name == 'leaky_relu'):\n",
    "        return np.where(x > 0, 1, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of input, hidden, and output neurons\n",
    "input_neurons = X_train.shape[1]\n",
    "l1_neurons = 3\n",
    "l2_neurons = 100\n",
    "l3_neurons = 20 \n",
    "output_neurons = 1  # Linear activation for regression\n",
    "\n",
    "# Initialize the weights with random values in range (-1,1)\n",
    "np.random.seed(1)\n",
    "w1 = 2 * np.random.random((input_neurons, l1_neurons)) - 1\n",
    "w2 = 2 * np.random.random((l1_neurons, l2_neurons)) - 1\n",
    "w3 = 2 * np.random.random((l2_neurons, l3_neurons)) - 1\n",
    "w4 = 2 * np.random.random((l3_neurons, output_neurons)) - 1\n",
    "\n",
    "# Initialize the biases with random values in range (-1,1) \n",
    "b1 = 2 * np.random.random(l1_neurons) - 1\n",
    "b2 = 2 * np.random.random(l2_neurons) - 1\n",
    "b3 = 2 * np.random.random(l3_neurons) - 1\n",
    "b4 = 2 * np.random.random(output_neurons) - 1\n",
    "\n",
    "#Initialize Adam Parameters\n",
    "sw1 = np.zeros((input_neurons, l1_neurons))\n",
    "vw1 = np.zeros((input_neurons, l1_neurons))\n",
    "vw_hat1 = np.zeros((input_neurons, l1_neurons))\n",
    "sw_hat1 = np.zeros((input_neurons, l1_neurons))\n",
    "\n",
    "sw2 = np.zeros((l1_neurons, l2_neurons))\n",
    "vw2 = np.zeros((l1_neurons, l2_neurons))\n",
    "vw_hat2 = np.zeros((l1_neurons, l2_neurons))\n",
    "sw_hat2 = np.zeros((l1_neurons, l2_neurons))\n",
    "\n",
    "sw3 = np.zeros((l2_neurons, l3_neurons))\n",
    "vw3 = np.zeros((l2_neurons, l3_neurons))\n",
    "vw_hat3 = np.zeros((l2_neurons, l3_neurons))\n",
    "sw_hat3 = np.zeros((l2_neurons, l3_neurons))\n",
    "\n",
    "sw4 = np.zeros((l3_neurons, output_neurons))\n",
    "vw4 = np.zeros((l3_neurons, output_neurons))\n",
    "vw_hat4 = np.zeros((l3_neurons, output_neurons))\n",
    "sw_hat4 = np.zeros((l3_neurons, output_neurons))\n",
    "\n",
    "sb1 = np.zeros((1, l1_neurons))\n",
    "vb1 = np.zeros((1, l1_neurons))\n",
    "vb_hat1 = np.zeros((1, l1_neurons))\n",
    "sb_hat1 = np.zeros((1, l1_neurons))\n",
    "\n",
    "sb2 = np.zeros((1, l2_neurons))\n",
    "vb2 = np.zeros((1, l2_neurons))\n",
    "vb_hat2 = np.zeros((1, l2_neurons))\n",
    "sb_hat2 = np.zeros((1, l2_neurons))\n",
    "\n",
    "sb3 = np.zeros((1, l3_neurons))\n",
    "vb3 = np.zeros((1, l3_neurons))\n",
    "vb_hat3 = np.zeros((1, l3_neurons))\n",
    "sb_hat3 = np.zeros((1, l3_neurons))\n",
    "\n",
    "sb4 = np.zeros((1, output_neurons))\n",
    "vb4 = np.zeros((1, output_neurons))\n",
    "vb_hat4 = np.zeros((1, output_neurons))\n",
    "sb_hat4 = np.zeros((1, output_neurons))\n",
    "\n",
    "epsilon = 0.00000001\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.002\n",
    "epochs = 500  # Train sample by sample \n",
    "\n",
    "mse_train = np.zeros(epochs)\n",
    "mse_test = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training the MLP for regression\n",
    "for epoch in range(1,epochs+1):\n",
    "\n",
    "    # Shuffle the training data in each epoch\n",
    "#     shuffle_indices = np.arange(len(X_train))\n",
    "#     np.random.shuffle(shuffle_indices)\n",
    "#     X_train = X_train[shuffle_indices]\n",
    "#     y_train = y_train[shuffle_indices]\n",
    "\n",
    "\n",
    "    total_error = 0\n",
    "    error_data_train = np.zeros(len(X_train))\n",
    "    output_data_train = np.zeros(len(X_train))\n",
    "    for i in range(len(X_train)):\n",
    "        \n",
    "        #-------------------------------- Feed Forward -------------------------------------\n",
    "        input_layer = X_train[i:i+1] \n",
    "        \n",
    "        net1 = np.dot(input_layer, w1)+b1  # net1 = x * w1 + b1\n",
    "        o1   = activation_function(net1) #  o1 = f(net1)\n",
    "        \n",
    "        net2 = np.dot(o1, w2)+b2 # net2 = o1 * w2 + b2\n",
    "        o2   = activation_function(net2) #  o2 = f(net2)\n",
    "        \n",
    "        net3 = np.dot(o2, w3) +b3   # net3 = o2 * w3 + b3\n",
    "        o3   = activation_function(net3) #  o3 = f(net3)\n",
    "        \n",
    "        net4 = np.dot(o3, w4)+b4   # net4 = o3 * w4+b4\n",
    "        o4   =  net4      #  o4 = net4 # Linear activation for regression \n",
    "        output_data_train[i] = o4\n",
    "        \n",
    "        #-------------------------------- Backpropagation ----------------------------------- \n",
    "        output_layer_error = y_train[i:i+1] - o4\n",
    "        \n",
    "        # update w4\n",
    "        # dE/dw4 = dE/de * de/do4 * do4/dnet4 * dnet4/dw4 = e * -1 * fprim_net4 * o3 \n",
    "        w4_old = w4 \n",
    "        \n",
    "        dw4 = -1 * o3.T.dot(output_layer_error)\n",
    "\n",
    "        vw4 = beta1 * vw4 + (1 - beta1) * dw4\n",
    "        vw_hat4 = vw4/(1-beta1**epoch)\n",
    "\n",
    "        sw4 = beta2 * sw4 +(1-beta2) * (dw4 ** 2)\n",
    "        sw_hat4 = sw4/(1-beta2**epoch)\n",
    "        \n",
    "        w4 = w4 - (learning_rate / (np.sqrt(sw_hat4)+epsilon))* vw_hat4\n",
    "        \n",
    "        # update b4\n",
    "        # dE/db4 = dE/de * de/do4 * do4/dnet4 * dnet4/db4 = e * -1 * fprim_net4 * 1\n",
    "        db4 = -1 * output_layer_error * 1\n",
    "        vb4 = beta1 * vb4 + (1 - beta1) * db4\n",
    "        vb_hat4 = vb4/(1-beta1**epoch)\n",
    "\n",
    "        sb4 = beta2 * sb4 +(1-beta2) * (db4 ** 2)\n",
    "        sb_hat4 = sb4/(1-beta2**epoch)\n",
    "        \n",
    "        b4 = b4 - (learning_rate / (np.sqrt(sb_hat4)+epsilon))* vb_hat4\n",
    "        \n",
    "        # update w3\n",
    "        # dE/dw3 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/dw3 = e * -1 * fprim_net4 * w4 * fprim_net3 * o2\n",
    "        w3_old = w3\n",
    "        # Create a diagonal matrix\n",
    "        fprim_net3 = np.array(activation_function_derivative(net3))[0] \n",
    "        diag_matrix_fprim_net3 = np.diag(fprim_net3)\n",
    "        dw3 = -1 * output_layer_error * o2.T.dot(np.dot(w4_old.T,diag_matrix_fprim_net3))\n",
    "        \n",
    "        vw3 = beta1 * vw3 + (1 - beta1) * dw3\n",
    "        vw_hat3 = vw3/(1-beta1**epoch)\n",
    "        \n",
    "        sw3 = beta2 * sw3 +(1-beta2) * (dw3 ** 2)\n",
    "        sw_hat3 = sw3/(1-beta2**epoch)\n",
    "                \n",
    "        w3 = w3 - (learning_rate / (np.sqrt(sw_hat3)+epsilon))* vw_hat3\n",
    "        \n",
    "        # update b3\n",
    "        # dE/db3 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/db3 = e * -1 * fprim_net4 * w4 * fprim_net3 * 1\n",
    "        db3 = -1 * learning_rate * output_layer_error * np.dot(w4_old.T,diag_matrix_fprim_net3) * 1\n",
    "        \n",
    "        vb3 = beta1 * vb3 + (1 - beta1) * db3\n",
    "        vb_hat3 = vb3/(1-beta1**epoch)\n",
    "        \n",
    "        sb3 = beta2 * sb3 +(1-beta2) * (db3 ** 2)\n",
    "        sb_hat3 = sb3/(1-beta2**epoch)\n",
    "                \n",
    "        b3 = b3 - (learning_rate / (np.sqrt(sb_hat3)+epsilon))* vb_hat3\n",
    "        \n",
    "        # update w2\n",
    "        # dE/dw2 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/dw2 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * o1\n",
    "        w2_old = w2 \n",
    "        fprim_net2 = np.array(activation_function_derivative(net2))[0]\n",
    "        diag_matrix_fprim_net2 = np.diag(fprim_net2) \n",
    "        dw2 = -1 * output_layer_error * o1.T.dot(np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)))\n",
    "        \n",
    "        vw2 = beta1 * vw2 + (1 - beta1) * dw2\n",
    "        vw_hat2 = vw2/(1-beta1**epoch)\n",
    "        \n",
    "        sw2 = beta2 * sw2 +(1-beta2) * (dw2 ** 2)\n",
    "        sw_hat2 = sw2/(1-beta2**epoch)\n",
    "                \n",
    "        w2 = w2 - (learning_rate / (np.sqrt(sw_hat2)+epsilon))* vw_hat2\n",
    "        \n",
    "        # update b2\n",
    "        # dE/db2 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/db2 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * 1\n",
    "        db2 =  -1 * learning_rate * output_layer_error * np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)) * 1\n",
    "        \n",
    "        vb2 = beta1 * vb2 + (1 - beta1) * db2\n",
    "        vb_hat2 = vb2/(1-beta1**epoch)\n",
    "        \n",
    "        sb2 = beta2 * sb2 +(1-beta2) * (db2 ** 2)\n",
    "        sb_hat2 = sb2/(1-beta2**epoch)\n",
    "                \n",
    "        b2 = b2 - (learning_rate / (np.sqrt(sb_hat2)+epsilon))* vb_hat2\n",
    "        \n",
    "        # update w1\n",
    "        # dE/dw1 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/do1 * do1/dnet1 * dnet1/dw1 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 * input\n",
    "        w1_old = w1        # Create a diagonal matrix \n",
    "        fprim_net1 = np.array(activation_function_derivative(net1))[0]\n",
    "        diag_matrix_fprim_net1 = np.diag(fprim_net1)\n",
    "        dw1 = -1 * learning_rate * output_layer_error * input_layer.T.dot(np.dot(np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)),np.dot(w2_old.T,diag_matrix_fprim_net1)))\n",
    "        \n",
    "        vw1 = beta1 * vw1 + (1 - beta1) * dw1\n",
    "        vw_hat1 = vw1/(1-beta1**epoch)\n",
    "        \n",
    "        sw1 = beta2 * sw1 +(1-beta2) * (dw1 ** 2)\n",
    "        sw_hat1 = sw1/(1-beta2**epoch)\n",
    "                \n",
    "        w1 = w1 - (learning_rate / (np.sqrt(sw_hat1)+epsilon))* vw_hat1\n",
    "        \n",
    "        # update b1\n",
    "        # dE/db1 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/do1 * do1/dnet1 * dnet1/db1 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 * 1\n",
    "        db1 = -1 * learning_rate * output_layer_error * np.dot(np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)),np.dot(w2_old.T,diag_matrix_fprim_net1)) * 1\n",
    "        \n",
    "        vb1 = beta1 * vb1 + (1 - beta1) * db1\n",
    "        vb_hat1 = vb1/(1-beta1**epoch)\n",
    "        \n",
    "        sb1 = beta2 * sb1 +(1-beta2) * (db1 ** 2)\n",
    "        sb_hat1 = sb1/(1-beta2**epoch)\n",
    "                \n",
    "        b1 = b1 - (learning_rate / (np.sqrt(sb_hat1)+epsilon))* vb_hat1\n",
    "        \n",
    "        error_data_train[i] = output_layer_error\n",
    "        total_error += np.abs(output_layer_error)\n",
    "    \n",
    "    mse_train[epoch-1] = np.mean(error_data_train ** 2)\n",
    "    \n",
    "    # Testing the trained MLP for regression \n",
    "    error_data_test = np.zeros(len(X_test))\n",
    "    output_data_test = np.zeros(len(X_test))\n",
    "    for i in range(len(X_test)):\n",
    "        input_layer = X_test[i:i+1] \n",
    "        net1 = np.dot(input_layer, w1) +b1   # net1 = x * w1 +b1\n",
    "        o1   = activation_function(net1) #  o1 = f(net1)\n",
    "\n",
    "        net2 = np.dot(o1, w2) + b2 # net2 = o1 * w2 + b2 \n",
    "        o2   = activation_function(net2) #  o2 = f(net2)\n",
    "\n",
    "        net3 = np.dot(o2, w3) + b3   # net3 = o2 * w3 + b3\n",
    "        o3   = activation_function(net3) #  o3 = f(net3)\n",
    "\n",
    "        net4 = np.dot(o3, w4) + b4  # net4 = o3 * w4 + b4\n",
    "        o4   =  net4       \n",
    "    \n",
    "        output_data_test[i] = o4\n",
    "        error_data_test[i] = y_test[i] - o4 \n",
    "    \n",
    "    mse_test[epoch-1] = np.mean(error_data_test ** 2)\n",
    "        \n",
    "    # Plotting the training data and output\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(y_train)\n",
    "    plt.plot(output_data_train, 'r', linewidth=0.5)\n",
    "    plt.xlabel('Train Data')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "    # Plotting the training MSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.semilogy(np.arange(1, epoch ), mse_train[:epoch-1])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Train') \n",
    "    \n",
    "    print('Epoch: {} \\t'.format(epoch))\n",
    "    print('total_error: ',total_error)\n",
    "    print('MSE_train: {:.4f}'.format(mse_train[epoch-1]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "      \n",
    "    print(\"\\n\\033[1;m\" + \"*\" * 125)\n",
    "         \n",
    "        \n",
    "print(\"****************************** Training completed *******************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#because epoch starts from 1 to epochs+1\n",
    "epoch = epoch-1\n",
    "# Plotting the training data and output\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(y_train)\n",
    "plt.plot(output_data_train, 'r', linewidth=0.5)\n",
    "plt.xlabel('Train Data')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "# Plotting the training MSE \n",
    "plt.subplot(2, 2, 2)\n",
    "plt.semilogy(np.arange(1, epoch + 1), mse_train[:epoch])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Train') \n",
    "\n",
    "\n",
    "# Plotting the test data and output\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(y_test)\n",
    "plt.plot(output_data_test, 'r', linewidth=0.5)\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "# Plotting the test MSE\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.semilogy(np.arange(1, epoch + 1), mse_test[:epoch])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Test')  \n",
    "\n",
    "\n",
    "print('MSE_train: ',mse_train[epoch])\n",
    "print('MSE_test: ',mse_test[epoch])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "m_train , b_train = np.polyfit(y_train, output_data_train, 1)    \n",
    "plt.scatter(y_train, output_data_train,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(y_train, m_train*y_train+b_train,'r') \n",
    "plt.title('Regression Train') \n",
    "\n",
    "plt.figure(3)\n",
    "m_test , b_test = np.polyfit(y_test, output_data_test, 1)  \n",
    "plt.scatter(y_test, output_data_test,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(y_test, m_test*y_test+b_test,'r')\n",
    "plt.title('Regression Test')\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse_train_result = mse_train[-1]\n",
    "mse_test_result = mse_test[-1]\n",
    "\n",
    "print(\"Final MSE on Train Data:\", mse_train_result)\n",
    "print(\"Final MSE on Test Data:\", mse_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP4cHBUTTRrxiRZtggpjKCM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset and Split to Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Excel file\n",
    "# data = pd.read_excel('Temperature Dataset.xlsx', header=None).values\n",
    "data = pd.read_excel('ECG Datasets.xlsx', header=None).values\n",
    "# data = pd.read_excel('Lorenz Dataset.xlsx', header=None).values\n",
    "\n",
    "num_data = data.shape[0]\n",
    "\n",
    "# Normalize the input data\n",
    "for ii in range(4): \n",
    "    data[:, ii] = data[:, ii] / np.max(data[:, ii]) \n",
    "    \n",
    "# Split the dataset into a training and testing set\n",
    "# X_train, X_test,y_train, y_test = train_test_split(data[:num_data, :3], data[:num_data,3], test_size=0.25, random_state=42)\n",
    "\n",
    "split_ratio_train = 0.7\n",
    "\n",
    "split_line_number = int(np.shape(data)[0] * split_ratio_train)\n",
    "X_train = data[:split_line_number, :3]\n",
    "y_train = data[:split_line_number, 3]\n",
    "\n",
    "other_data = data[split_line_number:, :4]\n",
    "\n",
    "X_test = data[split_line_number:len(data), :3]\n",
    "y_test = data[split_line_number:len(data), 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_FUNC = 'leaky_relu'\n",
    "alpha = 0.01\n",
    "def activation_function(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.maximum(0, x)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        return  1 /( 1 + (math.e)**(-1 * x))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        return 2/(1+ (math.e)**(-2*x))-1\n",
    "    elif(fun_name == 'leaky_relu'): \n",
    "        return np.where(x > 0, x, alpha * x) \n",
    "\n",
    "def activation_function_derivative(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.where(x > 0, 1, 0)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        a = activation_function(x)\n",
    "        a = np.reshape(a, (-1,1))\n",
    "        b = 1 - activation_function(x)\n",
    "        b = np.reshape(b, (-1,1))\n",
    "        b = np.transpose(b)\n",
    "        return np.diag(np.diag(np.matmul(a,b)))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        tansig_x = activation_function(x)\n",
    "        return 1 - tansig_x**2\n",
    "    elif(fun_name == 'leaky_relu'):\n",
    "        return np.where(x > 0, 1, alpha)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of input, hidden, and output neurons\n",
    "input_neurons = X_train.shape[1]\n",
    "l1_neurons = 3\n",
    "l2_neurons = 100\n",
    "l3_neurons = 20 \n",
    "output_neurons = 1  # Linear activation for regression\n",
    "\n",
    "# Initialize the weights with random values in range (-1,1)\n",
    "np.random.seed(1)\n",
    "w1 = 2 * np.random.random((input_neurons, l1_neurons)) - 1\n",
    "w2 = 2 * np.random.random((l1_neurons, l2_neurons)) - 1\n",
    "w3 = 2 * np.random.random((l2_neurons, l3_neurons)) - 1\n",
    "w4 = 2 * np.random.random((l3_neurons, output_neurons)) - 1\n",
    "\n",
    "# Initialize the biases with random values in range (-1,1) \n",
    "b1 = 2 * np.random.random(l1_neurons) - 1\n",
    "b2 = 2 * np.random.random(l2_neurons) - 1\n",
    "b3 = 2 * np.random.random(l3_neurons) - 1\n",
    "b4 = 2 * np.random.random(output_neurons) - 1\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.002\n",
    "epochs = 500  # Train sample by sample \n",
    "\n",
    "mse_train = np.zeros(epochs)\n",
    "mse_test = np.zeros(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Training the MLP for regression\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Shuffle the training data in each epoch\n",
    "#     shuffle_indices = np.arange(len(X_train))\n",
    "#     np.random.shuffle(shuffle_indices)\n",
    "#     X_train = X_train[shuffle_indices]\n",
    "#     y_train = y_train[shuffle_indices]\n",
    "\n",
    "\n",
    "    total_error = 0\n",
    "    error_data_train = np.zeros(len(X_train))\n",
    "    output_data_train = np.zeros(len(X_train))\n",
    "    for i in range(len(X_train)):\n",
    "        \n",
    "        #-------------------------------- Feed Forward -------------------------------------\n",
    "        input_layer = X_train[i:i+1] \n",
    "        \n",
    "        net1 = np.dot(input_layer, w1)+b1  # net1 = x * w1 + b1\n",
    "        o1   = activation_function(net1) #  o1 = f(net1)\n",
    "        \n",
    "        net2 = np.dot(o1, w2)+b2 # net2 = o1 * w2 + b2\n",
    "        o2   = activation_function(net2) #  o2 = f(net2)\n",
    "        \n",
    "        net3 = np.dot(o2, w3) +b3   # net3 = o2 * w3 + b3\n",
    "        o3   = activation_function(net3) #  o3 = f(net3)\n",
    "        \n",
    "        net4 = np.dot(o3, w4)+b4   # net4 = o3 * w4+b4\n",
    "        o4   =  net4      #  o4 = net4 # Linear activation for regression \n",
    "        output_data_train[i] = o4\n",
    "        \n",
    "        #-------------------------------- Backpropagation ----------------------------------- \n",
    "        output_layer_error = y_train[i:i+1] - o4\n",
    "        \n",
    "        # update w4\n",
    "        # dE/dw4 = dE/de * de/do4 * do4/dnet4 * dnet4/dw4 = e * -1 * fprim_net4 * o3 \n",
    "        w4_old = w4 \n",
    "        w4 = w4 +  learning_rate * o3.T.dot(output_layer_error)\n",
    "        \n",
    "        # update b4\n",
    "        # dE/db4 = dE/de * de/do4 * do4/dnet4 * dnet4/db4 = e * -1 * fprim_net4 * 1\n",
    "        b4 = b4 +  learning_rate * output_layer_error * 1\n",
    "        \n",
    "        # update w3\n",
    "        # dE/dw3 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/dw3 = e * -1 * fprim_net4 * w4 * fprim_net3 * o2\n",
    "        w3_old = w3\n",
    "        # Create a diagonal matrix\n",
    "        fprim_net3 = np.array(activation_function_derivative(net3))[0] \n",
    "        diag_matrix_fprim_net3 = np.diag(fprim_net3)\n",
    "        w3 = w3 + learning_rate * output_layer_error * o2.T.dot(np.dot(w4_old.T,diag_matrix_fprim_net3))\n",
    "        \n",
    "        # update b3\n",
    "        # dE/db3 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/db3 = e * -1 * fprim_net4 * w4 * fprim_net3 * 1\n",
    "        b3 = b3 + learning_rate * output_layer_error * np.dot(w4_old.T,diag_matrix_fprim_net3) * 1\n",
    "        \n",
    "        # update w2\n",
    "        # dE/dw2 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/dw2 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * o1\n",
    "        w2_old = w2 \n",
    "        fprim_net2 = np.array(activation_function_derivative(net2))[0]\n",
    "        diag_matrix_fprim_net2 = np.diag(fprim_net2) \n",
    "        w2 = w2 + learning_rate * output_layer_error * o1.T.dot(np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)))\n",
    "        \n",
    "        # update b2\n",
    "        # dE/db2 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/db2 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * 1\n",
    "        b2 = b2 + learning_rate * output_layer_error * np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)) * 1\n",
    "                 \n",
    "        # update w1\n",
    "        # dE/dw1 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/do1 * do1/dnet1 * dnet1/dw1 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 * input\n",
    "        w1_old = w1        # Create a diagonal matrix \n",
    "        fprim_net1 = np.array(activation_function_derivative(net1))[0]\n",
    "        diag_matrix_fprim_net1 = np.diag(fprim_net1)\n",
    "        w1 = w1 + learning_rate * output_layer_error * input_layer.T.dot(np.dot(np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)),np.dot(w2_old.T,diag_matrix_fprim_net1)))\n",
    "        \n",
    "        # update b1\n",
    "        # dE/db1 = dE/de * de/do4 * do4/dnet4 * dnet4/do3 * do3/dnet3 * dnet3/do2 * do2/dnet2 * dnet2/do1 * do1/dnet1 * dnet1/db1 = e * -1 * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 * 1\n",
    "        b1 = b1 + learning_rate * output_layer_error * np.dot(np.dot(np.dot(w4_old.T,diag_matrix_fprim_net3),np.dot(w3_old.T,diag_matrix_fprim_net2)),np.dot(w2_old.T,diag_matrix_fprim_net1)) * 1\n",
    "        \n",
    "        error_data_train[i] = output_layer_error\n",
    "        total_error += np.abs(output_layer_error)\n",
    "    \n",
    "    mse_train[epoch] = np.mean(error_data_train ** 2)\n",
    "    \n",
    "    # Testing the trained MLP for regression \n",
    "    error_data_test = np.zeros(len(X_test))\n",
    "    output_data_test = np.zeros(len(X_test))\n",
    "    for i in range(len(X_test)):\n",
    "        input_layer = X_test[i:i+1] \n",
    "        net1 = np.dot(input_layer, w1) +b1   # net1 = x * w1 +b1\n",
    "        o1   = activation_function(net1) #  o1 = f(net1)\n",
    "\n",
    "        net2 = np.dot(o1, w2) + b2 # net2 = o1 * w2 + b2 \n",
    "        o2   = activation_function(net2) #  o2 = f(net2)\n",
    "\n",
    "        net3 = np.dot(o2, w3) + b3   # net3 = o2 * w3 + b3\n",
    "        o3   = activation_function(net3) #  o3 = f(net3)\n",
    "\n",
    "        net4 = np.dot(o3, w4) + b4  # net4 = o3 * w4 + b4\n",
    "        o4   =  net4       \n",
    "    \n",
    "        output_data_test[i] = o4\n",
    "        error_data_test[i] = y_test[i] - o4 \n",
    "    \n",
    "    mse_test[epoch] = np.mean(error_data_test ** 2)\n",
    "        \n",
    "    # Plotting the training data and output\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(y_train)\n",
    "    plt.plot(output_data_train, 'r', linewidth=0.5)\n",
    "    plt.xlabel('Train Data')\n",
    "    plt.ylabel('Output')\n",
    "    plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "    # Plotting the training MSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.semilogy(np.arange(1, epoch + 1), mse_train[:epoch])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Train') \n",
    "    \n",
    "    print('Epoch: {} \\t'.format(epoch+1))\n",
    "    print('total_error: ',total_error)\n",
    "    print('MSE_train: {:.4f}'.format(mse_train[epoch]))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "      \n",
    "    print(\"\\n\\033[1;m\" + \"*\" * 125)\n",
    "         \n",
    "        \n",
    "print(\"****************************** Training completed *******************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plotting the training data and output\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(y_train)\n",
    "plt.plot(output_data_train, 'r', linewidth=0.5)\n",
    "plt.xlabel('Train Data')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "# Plotting the training MSE \n",
    "plt.subplot(2, 2, 2)\n",
    "plt.semilogy(np.arange(1, epoch + 1), mse_train[:epoch])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Train') \n",
    "\n",
    "\n",
    "# Plotting the test data and output\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(y_test)\n",
    "plt.plot(output_data_test, 'r', linewidth=0.5)\n",
    "plt.xlabel('Test Data')\n",
    "plt.ylabel('Output')\n",
    "plt.legend(['Actual', 'Predicted'])\n",
    "\n",
    "# Plotting the test MSE\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.semilogy(np.arange(1, epoch + 1), mse_test[:epoch])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Test')  \n",
    "\n",
    "\n",
    "print('MSE_train: ',mse_train[epoch])\n",
    "print('MSE_test: ',mse_test[epoch]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "m_train , b_train = np.polyfit(y_train, output_data_train, 1)    \n",
    "plt.scatter(y_train, output_data_train,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(y_train, m_train*y_train+b_train,'r') \n",
    "plt.title('Regression Train') \n",
    "\n",
    "plt.figure(3)\n",
    "m_test , b_test = np.polyfit(y_test, output_data_test, 1)  \n",
    "plt.scatter(y_test, output_data_test,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(y_test, m_test*y_test+b_test,'r')\n",
    "plt.title('Regression Test')\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "mse_train_result = mse_train[-1]\n",
    "mse_test_result = mse_test[-1]\n",
    "\n",
    "print(\"Final MSE on Train Data:\", mse_train_result)\n",
    "print(\"Final MSE on Test Data:\", mse_test_result)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyP4cHBUTTRrxiRZtggpjKCM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

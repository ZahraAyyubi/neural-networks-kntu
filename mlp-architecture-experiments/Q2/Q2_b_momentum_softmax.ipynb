{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ebafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.utils import shuffle \n",
    "\n",
    "# Load data from Excel file\n",
    "data = pd.read_excel('classification-seeds.xlsx', header=None).values\n",
    "# data = pd.read_excel('classification-iris.xlsx', header=None).values\n",
    "# data = pd.read_excel('dataset_diabetic.xlsx', header=None).values\n",
    "\n",
    "# Shuffle the data\n",
    "data = shuffle(data, random_state=42)\n",
    "\n",
    "num_data = data.shape[0]\n",
    "num_col = data.shape[1]\n",
    "num_class = len(np.unique(data[:,num_col-1]))\n",
    "\n",
    "# Normalize the input data\n",
    "for ii in range(num_col-1):\n",
    "    data[:, ii] = data[:, ii] / np.max(data[:, ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228d77c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "percent_train = 0.7\n",
    "num_train = round(num_data * percent_train)\n",
    "num_test = num_data - num_train\n",
    "\n",
    "# Convert labels to one-hot encoding (necessary for multi-class classification)\n",
    "# num_classes = len(np.unique(y_train))\n",
    "y_one_hot_train = np.zeros((num_train, num_class))\n",
    "y_one_hot_train[np.arange(num_train), data[:num_train, num_col-1].astype(int)-1] = 1\n",
    "\n",
    "y_one_hot_test = np.zeros((num_test, num_class))\n",
    "y_one_hot_test[np.arange(num_test), data[num_train:, num_col-1].astype(int)-1] = 1\n",
    "\n",
    "# for iris\n",
    "# n0 = data.shape[1]-1\n",
    "# n1 = 15\n",
    "# n2 = 10\n",
    "# n3 = 5\n",
    "# n4 = num_class\n",
    "\n",
    "# for seed\n",
    "n0 = data.shape[1]-1\n",
    "n1 = 15\n",
    "n2 = 8\n",
    "n3 = 5\n",
    "n4 = num_class \n",
    "\n",
    "learning_rate1 = 0.02\n",
    "learning_rate2 = 0.02\n",
    "learning_rate3 = 0.02\n",
    "learning_rate4 = 0.0002 #seed\n",
    "# learning_rate4 = 0.002 #iris\n",
    "\n",
    "\n",
    "# epoch = 35 #iris\n",
    "epoch = 45 #seed\n",
    "\n",
    "\n",
    "mse_train = np.zeros((epoch,num_class))\n",
    "mse_test = np.zeros((epoch,num_class)) \n",
    "\n",
    "output_data_train = np.zeros(epoch)\n",
    "output_data_test = np.zeros(epoch)\n",
    "\n",
    "a = -1\n",
    "b = 1\n",
    "\n",
    "# Initialize weights\n",
    "np.random.seed(1)\n",
    "w1 = np.random.uniform(a, b, size=(n1, n0))\n",
    "w2 = np.random.uniform(a, b, size=(n2, n1))\n",
    "w3 = np.random.uniform(a, b, size=(n3, n2))\n",
    "w4 = np.random.uniform(a, b, size=(n4, n3)) \n",
    "\n",
    "# Initialize biases\n",
    "b1 = np.random.uniform(a, b, size=(n1,1))\n",
    "b2 = np.random.uniform(a, b, size=(n2,1))\n",
    "b3 = np.random.uniform(a, b, size=(n3,1))\n",
    "b4 = np.random.uniform(a, b, size=(n4,1)) \n",
    "\n",
    "# Initialize momentum parameters \n",
    "beta = 0.9\n",
    "vw1 = np.zeros((n1, n0))\n",
    "vw2 = np.zeros((n2, n1))\n",
    "vw3 = np.zeros((n3, n2))\n",
    "vw4 = np.zeros((n4, n3))\n",
    "\n",
    "vb1 = np.zeros((n1,1))\n",
    "vb2 = np.zeros((n2,1))\n",
    "vb3 = np.zeros((n3,1))\n",
    "vb4 = np.zeros((n4,1))\n",
    "\n",
    "# Regularization parameters \n",
    "lambda_reg =0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fce0784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the softmax output of a vector\n",
    "# epsilon to avoid divide by zero encountered\n",
    "epsilon = 0.000001\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    sum = exp_z.sum() \n",
    "    softmax_z = np.round(exp_z/(sum +epsilon ),3)\n",
    "    return softmax_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df97a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for t in range(epoch): \n",
    "    error_data_train = np.zeros(num_train)\n",
    "    output_data_train = np.zeros(num_train) \n",
    "    \n",
    "    for i in range(num_train):\n",
    "        # feed-forward\n",
    "        input_data = data[i, :num_col-1].reshape(-1,1)\n",
    "        \n",
    "        net1 = w1 @ input_data + b1\n",
    "        o1 = (2/(1 + np.exp(-net1 ** 2))-1).reshape(-1,1)\n",
    "        net2 = w2 @ o1 + b2\n",
    "        o2 = (2 / (1 + np.exp(-net2 ** 2)) -1).reshape(-1,1)\n",
    "        net3 = w3 @ o2 + b3\n",
    "        o3 = (2 / (1 + np.exp(-net3 ** 2)) - 1).reshape(-1,1)\n",
    "        net4 = w4 @ o3 + b4\n",
    "#         o4 =  (1 / (1 + np.exp(-net4))).reshape(-1,1)  \n",
    "        o4 = net4\n",
    "        z = softmax(o4) \n",
    "        output_data_train[i] = np.argmax(z)+1\n",
    "        \n",
    "        # Backpropagation for a single sample \n",
    "        output_layer_error = -(y_one_hot_train[i:i+1] @ np.log10(z,where=z>0)).reshape(-1,1).flatten() # where=z>0 to avoid RuntimeWarning: divide by zero encountered in log10\n",
    " \n",
    "        # update w4 with momentum\n",
    "        w4_old = w4\n",
    "        # dw4 = dE/dz*dz/do4*do4/dnet4*dnet4/dw4 = (z - target) * fprim_net4 * o3\n",
    "        dw4 =  (z - y_one_hot_train[i:i+1].reshape(-1,1)) @ o3.reshape(1,-1) \n",
    "        vw4 = beta * vw4 + (1 - beta) * dw4 \n",
    "        w4 = w4 - learning_rate4 * vw4 - lambda_reg * w4\n",
    "        \n",
    "        # update b4 with momentum \n",
    "        # db4 = dE/dz*dz/do4*do4/dnet4*dnet4/db4 = (z - target) * fprim_net4 * 1 \n",
    "        db4 = (z - y_one_hot_train[i:i+1].reshape(-1,1))\n",
    "        vb4 = beta * vb4 + (1 - beta) * db4 \n",
    "        b4 = b4 - learning_rate4 * vb4 - lambda_reg * b4\n",
    "        \n",
    "        # update w3 with momentum\n",
    "        w3_old = w3\n",
    "        # dw3 = dE/z*dz/do4*do4/dnet4*dnet4/do3*do3/dnet3*dnet3/dw3 = (z - target) * fprim_net4 * w4 * fprim_net3 * o2\n",
    "        diag_fprim_net3 = np.diag((1-o3 ** 2).flatten())  \n",
    "        dw3 = diag_fprim_net3 @ w4.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1)) @  o2.reshape(1,-1) \n",
    "        vw3 = beta * vw3 + (1 - beta) * dw3 \n",
    "        w3 = w3 - learning_rate3 * vw3 - lambda_reg * w3\n",
    "        \n",
    "        # update b3 with momentum \n",
    "        # db3 =  dE/z*dz/do4*do4/dnet4*dnet4/do3*do3/dnet3*dnet3/db3 = (z - target) * fprim_net4 * w4 * fprim_net3 * 1\n",
    "        db3 = diag_fprim_net3 @ w4_old.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1))\n",
    "        vb3 = beta * vb3 + (1 - beta) * db3 \n",
    "        b3 = b3 - learning_rate3 * vb3 - lambda_reg * b3\n",
    "        \n",
    "        # update w2 with momentum   \n",
    "        w2_old = w2\n",
    "        diag_fprim_net2 = np.diag((1-o2 ** 2).flatten())\n",
    "        # dw2 = dE/dz*dz/do4*do4/dnet4*dnet4/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/dw2 = (z - target) * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 *o1\n",
    "        dw2 = diag_fprim_net2 @ w3_old.T @ diag_fprim_net3 @ w4_old.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1)) @ o1.reshape(1,-1) \n",
    "        vw2 = beta * vw2 + (1 - beta) * dw2 \n",
    "        w2 = w2 - learning_rate2 * vw2 - lambda_reg * w2\n",
    "    \n",
    "        # update b2 with momentum   \n",
    "        # db2 = dE/dz*dz/do4*do4/dnet4*dnet4/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/db2 = (z - target) * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * 1\n",
    "        db2 = diag_fprim_net2 @ w3_old.T @ diag_fprim_net3 @ w4_old.T @ (z - y_one_hot_train[i:i+1].reshape(-1,1)) \n",
    "        vb2 = beta * vb2 + (1 - beta) * db2 \n",
    "        b2 = b2 - learning_rate2 * vb2 - lambda_reg * b2\n",
    "        \n",
    "        # update w1 with momentum    \n",
    "        # dw1 = dE/dz*dz/do4*do4/dnet4*dnet4/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/dw1 = (z - target) * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 * x\n",
    "        diag_fprim_net1 = np.diag((1-o1 ** 2).flatten())\n",
    "        dw1 = diag_fprim_net1 @ w2_old.T @ diag_fprim_net2 @  w3_old.T @  diag_fprim_net3 @ w4_old.T @  (z - y_one_hot_train[i:i+1].reshape(-1,1)) @ input_data.reshape(1,-1)\n",
    "        vw1 = beta * vw1 + (1 - beta) * dw1\n",
    "        w1 = w1 - learning_rate1 * vw1 - lambda_reg * w1\n",
    "        \n",
    "        # update b1 with momentum    \n",
    "        # db1 = dE/dz*dz/do4*do4/dnet4*dnet4/do3*do3/dnet3*dnet3/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/db1 = (z - target) * fprim_net4 * w4 * fprim_net3 * w3 * fprim_net2 * w2 * fprim_net1 \n",
    "        db1 = diag_fprim_net1 @ w2_old.T @ diag_fprim_net2 @  w3_old.T @  diag_fprim_net3 @ w4_old.T @  (z - y_one_hot_train[i:i+1].reshape(-1,1))\n",
    "        vb1 = beta * vb1 + (1 - beta) * db1\n",
    "        b1 = b1 - learning_rate1 * vb1 - lambda_reg * b1\n",
    "    \n",
    "        error_data_train[i] = output_layer_error[0]\n",
    "\n",
    "    mse_train[t] = np.mean(error_data_train ** 2,axis=0) \n",
    "    \n",
    "    error_data_test = np.zeros(num_test)\n",
    "    output_data_test = np.zeros(num_test)\n",
    "    for i in range(num_test):\n",
    "        input_data = data[num_train + i, :num_col-1].reshape(-1,1) \n",
    "        net1 = w1 @ input_data + b1 \n",
    "        o1 = (2/(1 + np.exp(-net1 ** 2))-1).reshape(-1,1) \n",
    "        net2 = w2 @ o1 + b2\n",
    "        o2 = (2 / (1 + np.exp(-net2 ** 2)) -1).reshape(-1,1)\n",
    "        net3 = w3 @ o2 + b3\n",
    "        o3 = (2 / (1 + np.exp(-net3 ** 2)) - 1).reshape(-1,1)\n",
    "        net4 = w4 @ o3 + b4\n",
    "#         o4 =  (1 / (1 + np.exp(-net4))).reshape(-1,1)   \n",
    "        o4 = net4\n",
    "        z = softmax(o4) \n",
    "        output_data_test[i] = np.argmax(z)+1\n",
    "        \n",
    "        error = -(y_one_hot_test[i:i+1] @ np.log10(z,where=z>0)).flatten() \n",
    "\n",
    "        error_data_test[i] = error[0]\n",
    "\n",
    "    mse_test[t] = np.mean(error_data_test ** 2,axis=0)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(data[:num_train, num_col-1], '-sr')\n",
    "    plt.plot(output_data_train, '-*b')\n",
    "    plt.xlabel('Train Data')\n",
    "    plt.ylabel('Output')   \n",
    "    \n",
    "#         Plotting the training MSE\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.semilogy(np.arange(1, t + 1), mse_train[:t])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Train')\n",
    "    \n",
    "\n",
    "    plt.subplot(2, 2, 3)    \n",
    "    plt.plot(data[num_train:, num_col-1], '-sr')\n",
    "    plt.plot(output_data_test, '-*b')\n",
    "    plt.xlabel('Test Data')\n",
    "    plt.ylabel('Output')\n",
    "    \n",
    "        # Plotting the test MSE\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.semilogy(np.arange(1, t + 1), mse_test[:t])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MSE Test')\n",
    "\n",
    "    \n",
    "    print('Epoch: {} \\t'.format(t+1))\n",
    "    print('MSE_train: ',mse_train[t],' MSE_Test: ' ,mse_test[t])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\\033[1;m\" + \"*\" * 125)\n",
    "    \n",
    "    \n",
    "plt.figure(2)\n",
    "m_train , b_train = np.polyfit(data[:num_train, num_col - 1], output_data_train, 1)    \n",
    "plt.scatter(data[:num_train, num_col - 1], output_data_train,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(data[:num_train, num_col - 1], m_train*data[:num_train, num_col - 1]+b_train,'r')\n",
    "# plt.plot(data[:num_train, 3], output_data_train, label='Regression Line', color='red')\n",
    "plt.title('Regression Train') \n",
    "\n",
    "plt.figure(3)\n",
    "m_test , b_test = np.polyfit(data[num_train:, num_col - 1], output_data_test, 1)  \n",
    "plt.scatter(data[num_train:, num_col - 1], output_data_test,facecolors='none',edgecolors='#104E8B')\n",
    "plt.plot(data[num_train:, num_col - 1], m_test*data[num_train:,num_col - 1]+b_test,'r')\n",
    "plt.title('Regression Test')\n",
    " \n",
    "mse_train_result = mse_train[-1]\n",
    "mse_test_result = mse_test[-1]\n",
    "\n",
    "print(\"Final MSE on Train Data:\", mse_train_result)\n",
    "print(\"Final MSE on Test Data:\", mse_test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8ede1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have converted regression predictions to classes\n",
    "predicted_classes = output_data_train \n",
    "actual_classes = data[:num_train,num_col-1] \n",
    "\n",
    "# Define the number of classes\n",
    "\n",
    "# Create a confusion matrix-like matrix\n",
    "confusion_matrix = np.zeros((num_class, num_class))\n",
    "\n",
    "# Fill the confusion matrix\n",
    "for actual, predicted in zip(actual_classes, predicted_classes):\n",
    "    confusion_matrix[actual.astype(int)-1][predicted.astype(int)-1] += 1\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Train Confusion Matrix ')\n",
    "plt.colorbar()\n",
    "\n",
    "# Annotate the plot with numbers\n",
    "for i in range(num_class):\n",
    "    for j in range(num_class):\n",
    "        plt.text(j, i, str(int(confusion_matrix[i, j])), fontsize=12, ha='center', va='center')  # Corrected\n",
    "\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.xticks(np.arange(num_class), np.arange(1, num_class + 1))\n",
    "plt.yticks(np.arange(num_class), np.arange(1, num_class + 1))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f788bf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Assuming you have converted regression predictions to classes\n",
    "predicted_classes = output_data_test \n",
    "actual_classes = data[num_train:,num_col-1] \n",
    "\n",
    "# Define the number of classes\n",
    "\n",
    "# Create a confusion matrix-like matrix\n",
    "confusion_matrix = np.zeros((num_class, num_class))\n",
    "\n",
    "# Fill the confusion matrix\n",
    "for actual, predicted in zip(actual_classes, predicted_classes):\n",
    "    confusion_matrix[actual.astype(int)-1][predicted.astype(int)-1] += 1\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "plt.title('Test Confusion Matrix ')\n",
    "plt.colorbar()\n",
    "\n",
    "# Annotate the plot with numbers\n",
    "for i in range(num_class):\n",
    "    for j in range(num_class):\n",
    "        plt.text(j, i, str(int(confusion_matrix[i, j])), fontsize=12, ha='center', va='center')  # Corrected\n",
    "# Adjust ticks and labels to start from 1\n",
    "        \n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "# plt.xticks(np.arange(num_class))\n",
    "# plt.yticks(np.arange(num_class))\n",
    "plt.xticks(np.arange(num_class), np.arange(1, num_class + 1))\n",
    "plt.yticks(np.arange(num_class), np.arange(1, num_class + 1))\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f27108",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracy = np.mean(output_data_train == data[:num_train,num_col-1] ) \n",
    "print(f\"Accuracy on the train set: {train_accuracy * 100}%\")\n",
    "\n",
    "test_accuracy = np.mean(output_data_test == data[num_train:,num_col-1]) \n",
    "print(f\"Accuracy on the test set: {test_accuracy * 100}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

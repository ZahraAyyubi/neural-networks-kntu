# MLP Architecture Experiments

This repository includes complete Python implementations and analysis of multilayer perceptron (MLP) experiments conducted as part of the **Neural Networks graduate course** at *K. N. Toosi University of Technology*. The codebase is modular, organized by task, and structured for reuse and evaluation on a variety of datasets and activation functions.

---

## ğŸ§© Assignment Breakdown

- **Q1 â€“ Time-Series Regression with LM**  
  Training MLP with Levenbergâ€“Marquardt on ECG, Lorenz, TSE, and Temperature data

- **Q2 â€“ Classification with Activation Functions**  
  Testing various activations on datasets like iris, seeds, diabetic

- **Q3 â€“ Optimizer Comparison**  
  Comparison of Adam, LM, Momentum on identical tasks

- **Q4 â€“ Flexible Activation Functions**  
  Custom blended functions using tunable coefficients across layers (F1 & F2)

---
 
## ğŸ§ª Key Experiments and Findings

### âœ… Regression Results
Each model was evaluated using Mean Squared Error (MSE) on train/test splits (70/30). The Levenberg-Marquardt method showed good performance across all datasets. For example:
- With `tansig` activation, **lowest MSE** was obtained on `ECG` dataset.
- Adaptive methods like Adam showed **higher variance** but occasionally better convergence with proper tuning.

### âœ… Classification Results
- On `iris`, `seeds`, and `diabetic`, Levenberg-Marquardt with one-hot output and `tansig/logsig` activations achieved up to **82.2% test accuracy**.
- Momentum optimizer paired with `softmax + cross-entropy` loss showed **faster convergence** and **better stability** on small datasets.

### âœ… Flexible Activation Layers
Two variants were tested:
- **F1(x):** All neurons use the blended function Î±Â·logsig + Î²Â·tansig + Î³Â·ReLU + Î´Â·pureline
- **F2(x):** Only hidden layers use the blend; output uses `pureline`
> Findings: F1 provided better fitting capacity on regression tasks, especially for nonlinear patterns in Lorenz data.

---

## ğŸ” Highlights of Implementation

- MLPs are built and trained without using any ML libraries like PyTorch or Keras
- Each question handles its own input/output logic but reuses a central set of utility functions
- Flexible blend of nonlinearities in Q4 (controlled via `alpha`, `beta`, etc.)
- Architectures vary from 2 to 4 layers with bias/no-bias configurations
- Activation behavior, convergence, and MSE/accuracy are plotted for comparative analysis

---

## ğŸ—‚ï¸ Folder Structure

```
mlp-architecture-experiments/
â”œâ”€â”€ Q1/              â† Levenberg-Marquardt regression
â”œâ”€â”€ Q2/              â† Gradient descent & Momentum with various activations
â”œâ”€â”€ Q3/              â† Comparison: Momentum vs Levenberg vs Adam
â”œâ”€â”€ Q4/              â† Flexible Activation Functions (F1, F2)
â”œâ”€â”€ Report_Q1_Q2.xlsx â† Metrics and training logs for regression & classification
â”œâ”€â”€ Report_Q3_Q4.xlsx â† Comparative performance of optimizers and blended activation tests
```
 
## âš ï¸ Notes

- All experiments are reproducible via `main.py` files in each subfolder
- Outputs include `.csv` logs, `.png` plots, and summarized `.xlsx` reports
- Properly separated code for modeling, evaluation, plotting, and experimentation

# Gradient Descent Optimizers â€“ Neural Networks Course (Fall 2023)

This project was developed as part of the **Neural Networks course** at *K. N. Toosi University of Technology*. It includes:

- **Ex1:** Manual implementation and comparison of classical optimizers
- **Ex2:** Literature-based improvement using AdamX (variant of AMSGrad)

All algorithms were written from scratch in Python using only NumPy and Matplotlib.

---

## ğŸ“Œ Assignment Summary

### ğŸ”µ Ex1 â€“ Optimizer Implementation and Comparison

Implement the following optimizers on simple regression tasks (`y = wx + b`, `y = axÂ² + bx + c`):
- SGD, Momentum, Nesterov, AdaGrad, RMSProp, Adadelta, Adam, Adamax, Nadam, AMSGrad

Evaluate each methodâ€™s convergence behavior using MSE and analyze sensitivity to hyperparameters like `Î±` (learning rate), `Î²` (momentum), and `Îµ`.

### ğŸŸ¢ Ex2 â€“ Research-Based Extension

Reproduce and analyze the **AdamX** algorithm from the paper:  
*"On the Convergence Proof of AMSGrad and a New Version"*

Key improvement: introducing a **time-varying momentum parameter Î²(t)** to improve theoretical guarantees and convergence.

AdamX was tested on the same tasks and compared with AMSGrad using MSE and stability analysis.

---

## ğŸ§ª Key Insights from the Report

- **AdamX consistently outperformed AMSGrad** in both linear and quadratic regression (lower final MSE).
- **Adam**, **Nadam**, and **AMSGrad** also showed strong and stable performance.
- **AdaGrad** suffered from fast learning rate decay, leading to slow or stalled convergence.
- **SGD, Momentum, and Nesterov** required careful tuning and were sensitive to high or low learning rates.
- Hyperparameter tuning significantly influenced optimizer success:
  - Low Î± â†’ slow convergence
  - High Î± â†’ oscillation or divergence
  - Î² and Îµ also had nontrivial impact

---

## ğŸ“Š Optimizer Comparison Table

| Optimizer | Advantages | Disadvantages |
|-----------|------------|----------------|
| **SGD** | Simple, fast updates | Sensitive to learning rate, slow convergence |
| **Momentum** | Faster than SGD, smooths updates | Requires careful tuning of Î² |
| **Nesterov** | Anticipates gradient direction | More sensitive to overshooting |
| **AdaGrad** | Adapts learning rate per parameter | Learning rate decay too aggressive |
| **RMSProp** | Controls learning rate decay | Convergence sometimes unstable |
| **Adadelta** | Improves AdaGrad, no need to manually set learning rate | Slower on sparse data |
| **Adam** | Combines momentum + adaptivity, widely effective | Can overfit, sensitive to Îµ |
| **Adamax** | Stable on high-dimensional data | Slower than Adam in some tasks |
| **Nadam** | Combines Nesterov + Adam, good stability | Slightly slower than Adam |
| **AMSGrad** | Fixes convergence issue in Adam | Slightly slower update, more conservative |
| **AdamX** | Time-varying Î² improves stability & convergence | Slightly more complex to implement |

---

## ğŸ“ Folder Structure

```
gradient-descent-optimizers/
â”œâ”€â”€ Ex1/                            â† Optimizer implementations
â”‚   â”œâ”€â”€ ax+b/                       â† Linear regression (y = wx + b)
â”‚   â””â”€â”€ ax2+bx+c/                   â† Quadratic regression (y = axÂ² + bx + c)
â”œâ”€â”€ Ex2/                            â† Literature-based enhancement
â”‚   â”œâ”€â”€ ex2_ax+b.ipynb              â† AdamX vs AMSGrad (linear)
â”‚   â”œâ”€â”€ ex2_ax2+bx+c.ipynb          â† AdamX vs AMSGrad (quadratic)
â”‚   â””â”€â”€ AdamX.pdf                   â† Paper used for reference
â””â”€â”€ README.md                       â† This file
```

---

## âš ï¸ Notes

- No ML frameworks like TensorFlow or PyTorch were used.
- All optimizers implemented manually for educational purposes.
- MSE, parameter tuning, and plot analysis provided in report.

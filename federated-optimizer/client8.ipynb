{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ed50e0a",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9645f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.utils import shuffle \n",
    "import math\n",
    "import scipy\n",
    "import socket\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf0cae",
   "metadata": {},
   "source": [
    "# Initialize Client-Server Commuication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21863f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receive data until fully received (For Huge data => chunk by chunk)\n",
    "def receive_all(socket, length):\n",
    "    data = b''\n",
    "    while len(data) < length:\n",
    "        packet = socket.recv(length - len(data))\n",
    "        if not packet:\n",
    "            return None\n",
    "        data += packet\n",
    "    return data\n",
    "\n",
    "# Send data in chunks(For Huge data => chunk by chunk)\n",
    "def send_all(socket, data):\n",
    "    data_pickle = pickle.dumps(data)\n",
    "    data_size = len(data_pickle)\n",
    "    socket.sendall(data_size.to_bytes(4, 'big'))  # Send data size first\n",
    "\n",
    "    sent = 0\n",
    "    while sent < data_size:\n",
    "        chunk = data_pickle[sent:sent+4096]  # Send in chunks\n",
    "        socket.sendall(chunk)\n",
    "        sent += len(chunk)\n",
    "\n",
    "# Initialize client socket\n",
    "client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "server_address = ('localhost', 1111)\n",
    "client_socket.connect(server_address)\n",
    "print('Client socket initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5218e454",
   "metadata": {},
   "source": [
    "# Get Server Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe76c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receive initial params from server\n",
    "initial_parameters_size = int.from_bytes(client_socket.recv(4), 'big')  # Receive data size first\n",
    "initial_parameters_data = receive_all(client_socket, initial_parameters_size)\n",
    "initial_parameters = pickle.loads(initial_parameters_data)\n",
    "print('Received initial params from server')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e687dc5",
   "metadata": {},
   "source": [
    "# Load, Shuffle and Normalize Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f368d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data according to server defined data-mode\n",
    "data_mode = initial_parameters[\"data_mode\"]\n",
    "data = pd.read_excel(f'./client-datasets/{data_mode}/client-8.xlsx').values \n",
    "\n",
    "# Shuffle data\n",
    "data = shuffle(data, random_state=42)\n",
    "\n",
    "num_data = data.shape[0]\n",
    "num_col = data.shape[1] \n",
    "num_class = 10 # fashion-mnist\n",
    "\n",
    "# Normalize input data \n",
    "for ii in range(num_col-1): \n",
    "    data[:, ii] = ((data[:, ii] + 20) / 200)   # after test many  shift and scale value for data, with this values NN train better\n",
    "    \n",
    "percent_train = 0.7 # 70% Train 30% Test\n",
    "num_train = round(num_data * percent_train)\n",
    "num_test = num_data - num_train\n",
    "\n",
    "# Convert labels to one-hot encoding (necessary for multi-class classification) \n",
    "y_one_hot_train = np.zeros((num_train, num_class))\n",
    "y_one_hot_train[np.arange(num_train), data[:num_train, num_col-1].astype(int)] = 1\n",
    "\n",
    "y_one_hot_test = np.zeros((num_test, num_class))\n",
    "y_one_hot_test[np.arange(num_test), data[num_train:, num_col-1].astype(int)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1ef73f",
   "metadata": {},
   "source": [
    "# NN Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0b8736",
   "metadata": {},
   "outputs": [],
   "source": [
    "n0 = data.shape[1]-1\n",
    "n1 = 128\n",
    "n2 = num_class    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2021a91",
   "metadata": {},
   "source": [
    "# Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "server_rounds = initial_parameters[\"server_rounds\"] \n",
    "client_epochs = initial_parameters[\"client_epochs\"] \n",
    "batch_size = initial_parameters[\"batch_size\"]\n",
    "lambda_reg = initial_parameters[\"lambda_reg\"]\n",
    "\n",
    "# optimization params setting\n",
    "optimizer = initial_parameters[\"optimizer\"]\n",
    "optimization_params = initial_parameters[\"optimization_params\"] \n",
    "\n",
    "# initialize mse_test according to server_rounds (after each aggregation proccess evaluate global-model with local test-data)\n",
    "mse_test = np.zeros((server_rounds, len(optimization_params))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2b7c85",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab0969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVATION_FUNC = 'relu'\n",
    "leaky_relu_alpha = 0.01\n",
    "def activation_function(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.maximum(0, x)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        return  1 /( 1 + (math.e)**(-1 * x))\n",
    "    elif(fun_name == 'tansig'):\n",
    "        return 2/(1+ (math.e)**(-2*x))-1\n",
    "    elif(fun_name == 'leaky_relu'): \n",
    "        return np.where(x > 0, x, leaky_relu_alpha * x) \n",
    "\n",
    "def activation_function_derivative(x,fun_name=ACTIVATION_FUNC):\n",
    "    if(fun_name == 'relu'): \n",
    "        return np.where(x > 0, 1, 0)\n",
    "    elif(fun_name == 'logsig'): \n",
    "        logsig_x = activation_function(x)\n",
    "        return logsig_x * (1 - logsig_x)\n",
    "    elif(fun_name == 'tansig'):\n",
    "        tansig_x = activation_function(x)\n",
    "        return 1 - tansig_x**2\n",
    "    elif(fun_name == 'leaky_relu'):\n",
    "        return np.where(x > 0, 1, leaky_relu_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b358b9",
   "metadata": {},
   "source": [
    "# Softmax Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac4cec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z, axis=None):\n",
    "    exp_z = np.exp(z - np.max(z, axis=axis, keepdims=True))\n",
    "    softmax_z = exp_z / np.sum(exp_z, axis=axis, keepdims=True)\n",
    "    return softmax_z\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f962cab6",
   "metadata": {},
   "source": [
    "# FedSGD - Simple Minibatch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98e2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_sgd(data_shuffled, y_one_hot_shuffled,  learning_rate, decay, t, w1, w2):\n",
    "        error_data_train = np.zeros(num_train)\n",
    "        output_data_train = np.zeros(num_train)\n",
    "        \n",
    "        for i in range(0, num_train, batch_size):\n",
    "            # ******************************* feed-forward ******************************\n",
    "            batch_data = data_shuffled[i:i+batch_size].T \n",
    "            batch_labels = y_one_hot_shuffled[i:i+batch_size] \n",
    "            \n",
    "            net1 = w1 @ batch_data\n",
    "            o1 = activation_function(net1)\n",
    "            net2 = w2 @ o1\n",
    "            o2 = net2\n",
    "            z = softmax(o2,axis=0)\n",
    "            output_data_train[i:i+batch_size] = np.argmax(z, axis=0)\n",
    "            \n",
    "            # ****************************** Backpropagation for each mini-batch ****************************** \n",
    "            # cross entropy error\n",
    "            output_layer_error = np.mean(-(batch_labels.T * np.log(z, where=z>0)), axis=1)\n",
    "            \n",
    "            w2_old = w2\n",
    "            # dw2 = dE/dz*dz/do2*do2/dnet2*dnet2/dw2 = (z - target) * fprim_net2 * o1\n",
    "            dw2 = (z - batch_labels.T) @ o1.T / batch_size\n",
    "            w2 = w2 - (learning_rate/(1+decay*t)) * dw2 - lambda_reg * w2\n",
    "\n",
    "            fprim_net1 = activation_function_derivative(net1)\n",
    "            # dw1 = dE/z*dz/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/dw1 = (z - target) * fprim_net2 * w2 * fprim_net1 * input_data\n",
    "            dw1 = fprim_net1 * (w2_old.T @ (z - batch_labels.T)) @ batch_data.T / batch_size\n",
    "            w1 = w1 - (learning_rate/(1+decay*t)) * dw1 - lambda_reg * w1\n",
    "\n",
    "            error_data_train[i:i+batch_size] = output_layer_error.mean(axis=0)\n",
    "        \n",
    "        return {\"w1\": w1 ,\"w2\": w2,\"output_data_train\": output_data_train, \"error_data_train\": error_data_train}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ab6a6",
   "metadata": {},
   "source": [
    "# FedSGD + Nesterov momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06939539",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_nesterov_momentum(data_shuffled, y_one_hot_shuffled, learning_rate, decay, t, beta, w1, w2):\n",
    "    error_data_train = np.zeros(num_train)\n",
    "    output_data_train = np.zeros(num_train)\n",
    "    vw1 = np.zeros((n1, n0))\n",
    "    vw2 = np.zeros((n2, n1))\n",
    "\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        # ******************************* feed-forward ******************************\n",
    "        batch_data = data_shuffled[i:i+batch_size].T\n",
    "        batch_labels = y_one_hot_shuffled[i:i+batch_size]\n",
    "\n",
    "        # Nesterov Momentum update for w2\n",
    "        w2_tilde = w2 - beta * vw2\n",
    "        net1 = w1 @ batch_data\n",
    "        o1 = activation_function(net1)\n",
    "        net2 = w2_tilde @ o1\n",
    "        o2 = net2\n",
    "        z = softmax(o2, axis=0) \n",
    "        output_data_train[i:i+batch_size] = np.argmax(z, axis=0)\n",
    "        \n",
    "        # ****************************** Backpropagation for each mini-batch ****************************** \n",
    "        # cross entropy error\n",
    "        output_layer_error = np.mean(-(batch_labels.T * np.log(z, where=z > 0)), axis=1)\n",
    "        \n",
    "        # dw2* = dE/dz*dz/do2*do2/dnet2*dnet2/dw2 = (z - target) * fprim_net2 * o1\n",
    "        dw2_start = (z - batch_labels.T) @ o1.T / batch_size\n",
    "        vw2 = beta * vw2 + (1 - beta) * dw2_start\n",
    "        w2 = w2_tilde - (learning_rate / (1 + decay * t)) * vw2 - lambda_reg * w2\n",
    "\n",
    "        # Nesterov Momentum update for w1\n",
    "        fprim_net1 = activation_function_derivative(net1)\n",
    "        \n",
    "        # dw1* = dE/z*dz/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/dw1 = (z - target) * fprim_net2 * w2 * fprim_net1 * input_data\n",
    "        dw1_star = fprim_net1 * (w2_tilde.T @ (z - batch_labels.T)) @ batch_data.T / batch_size\n",
    "        vw1 = beta * vw1 + (1 - beta) * dw1_star\n",
    "        w1 = w1 - (learning_rate / (1 + decay * t)) * vw1 - lambda_reg * w1\n",
    "\n",
    "        error_data_train[i:i+batch_size] = output_layer_error.mean(axis=0)\n",
    "\n",
    "    return {\"w1\": w1, \"w2\": w2, \"output_data_train\": output_data_train, \"error_data_train\": error_data_train}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8e5fb5",
   "metadata": {},
   "source": [
    "# FedSGD + AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209af72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_adagrad(data_shuffled, y_one_hot_shuffled, learning_rate, decay, t, epsilon, w1, w2):\n",
    "    error_data_train = np.zeros(num_train)\n",
    "    output_data_train = np.zeros(num_train)\n",
    "    sw1 = np.zeros((n1, n0))\n",
    "    sw2 = np.zeros((n2, n1))\n",
    "\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        # ******************************* feed-forward ******************************\n",
    "        batch_data = data_shuffled[i:i+batch_size].T\n",
    "        batch_labels = y_one_hot_shuffled[i:i+batch_size]\n",
    "\n",
    "        net1 = w1 @ batch_data\n",
    "        o1 = activation_function(net1)\n",
    "        net2 = w2 @ o1\n",
    "        o2 = net2\n",
    "        z = softmax(o2, axis=0)\n",
    "        output_data_train[i:i+batch_size] = np.argmax(z, axis=0)\n",
    "        \n",
    "        # ****************************** Backpropagation for each mini-batch ****************************** \n",
    "        output_layer_error = np.mean(-(batch_labels.T * np.log(z, where=z > 0)), axis=1)\n",
    "        \n",
    "        w2_old = w2\n",
    "        # dw2 = dE/dz*dz/do2*do2/dnet2*dnet2/dw2 = (z - target) * fprim_net2 * o1\n",
    "        dw2 = (z - batch_labels.T) @ o1.T / batch_size\n",
    "        sw2 = sw2 + dw2 ** 2\n",
    "        w2 = w2 - ((learning_rate / (1 + decay * t)) / np.sqrt(sw2 + epsilon)) * dw2 - lambda_reg * w2\n",
    "        \n",
    "        fprim_net1 = activation_function_derivative(net1)\n",
    "        # dw1 = dE/z*dz/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/dw1 = (z - target) * fprim_net2 * w2 * fprim_net1 * input_data\n",
    "        dw1 = fprim_net1 * (w2_old.T @ (z - batch_labels.T)) @ batch_data.T / batch_size\n",
    "        sw1 = sw1 + dw1 ** 2\n",
    "        w1 = w1 - ((learning_rate / (1 + decay * t)) / np.sqrt(sw1 + epsilon)) * dw1 - lambda_reg * w1\n",
    "\n",
    "        error_data_train[i:i+batch_size] = output_layer_error.mean(axis=0)\n",
    "\n",
    "    return {\"w1\": w1, \"w2\": w2, \"output_data_train\": output_data_train, \"error_data_train\": error_data_train}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580b01d8",
   "metadata": {},
   "source": [
    "# FedSGD + RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3418ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_rmsprop(data_shuffled, y_one_hot_shuffled, learning_rate, decay, t, epsilon, beta, w1, w2):\n",
    "    error_data_train = np.zeros(num_train)\n",
    "    output_data_train = np.zeros(num_train)\n",
    "    sw1 = np.zeros((n1, n0))\n",
    "    sw2 = np.zeros((n2, n1))\n",
    "\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        # ******************************* feed-forward ******************************\n",
    "        batch_data = data_shuffled[i:i+batch_size].T\n",
    "        batch_labels = y_one_hot_shuffled[i:i+batch_size]\n",
    " \n",
    "        net1 = w1 @ batch_data\n",
    "        o1 = activation_function(net1)\n",
    "        net2 = w2 @ o1\n",
    "        o2 = net2\n",
    "        z = softmax(o2, axis=0)\n",
    "        output_data_train[i:i+batch_size] = np.argmax(z, axis=0)\n",
    "        \n",
    "        # ****************************** Backpropagation for each mini-batch ****************************** \n",
    "        output_layer_error = np.mean(-(batch_labels.T * np.log(z, where=z > 0)), axis=1)\n",
    "        \n",
    "        w2_old = w2\n",
    "        # dw2 = dE/dz*dz/do2*do2/dnet2*dnet2/dw2 = (z - target) * fprim_net2 * o1\n",
    "        dw2 = (z - batch_labels.T) @ o1.T / batch_size\n",
    "        sw2 = beta * sw2 +(1-beta) * (dw2 ** 2)\n",
    "        w2 = w2 - ((learning_rate / (1 + decay * t)) / np.sqrt(sw2 + epsilon)) * dw2 - lambda_reg * w2\n",
    "        \n",
    "        fprim_net1 = activation_function_derivative(net1)\n",
    "        # dw1 = dE/z*dz/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/dw1 = (z - target) * fprim_net2 * w2 * fprim_net1 * input_data\n",
    "        dw1 = fprim_net1 * (w2_old.T @ (z - batch_labels.T)) @ batch_data.T / batch_size\n",
    "        sw1 = beta * sw1 +(1-beta) * (dw1 ** 2)\n",
    "        w1 = w1 - ((learning_rate / (1 + decay * t)) / np.sqrt(sw1 + epsilon)) * dw1 - lambda_reg * w1\n",
    "\n",
    "        error_data_train[i:i+batch_size] = output_layer_error.mean(axis=0)\n",
    "\n",
    "    return {\"w1\": w1, \"w2\": w2, \"output_data_train\": output_data_train, \"error_data_train\": error_data_train}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e450c",
   "metadata": {},
   "source": [
    "# FedSGD + Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df91ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_adam(data_shuffled, y_one_hot_shuffled, learning_rate, decay, t, epsilon, beta1, beta2, w1, w2):\n",
    "    error_data_train = np.zeros(num_train)\n",
    "    output_data_train = np.zeros(num_train)\n",
    "    sw1 = np.zeros((n1, n0))\n",
    "    sw2 = np.zeros((n2, n1)) \n",
    "    \n",
    "    vw1 = np.zeros((n1, n0))\n",
    "    vw2 = np.zeros((n2, n1))\n",
    "    \n",
    "    vw1_hat = np.zeros((n1, n0))\n",
    "    vw2_hat = np.zeros((n2, n1))\n",
    "    \n",
    "    sw1_hat = np.zeros((n1, n0))\n",
    "    sw2_hat = np.zeros((n2, n1))\n",
    "\n",
    "    for i in range(0, num_train, batch_size):\n",
    "        # ******************************* feed-forward ******************************\n",
    "        batch_data = data_shuffled[i:i+batch_size].T\n",
    "        batch_labels = y_one_hot_shuffled[i:i+batch_size]\n",
    " \n",
    "        net1 = w1 @ batch_data\n",
    "        o1 = activation_function(net1)\n",
    "        net2 = w2 @ o1\n",
    "        o2 = net2\n",
    "        z = softmax(o2, axis=0)\n",
    "        output_data_train[i:i+batch_size] = np.argmax(z, axis=0)\n",
    "        \n",
    "        # ****************************** Backpropagation for each mini-batch ******************************\n",
    "        output_layer_error = np.mean(-(batch_labels.T * np.log(z, where=z > 0)), axis=1)\n",
    "        \n",
    "        w2_old = w2\n",
    "        # dw2 = dE/dz*dz/do2*do2/dnet2*dnet2/dw2 = (z - target) * fprim_net2 * o1\n",
    "        dw2 = (z - batch_labels.T) @ o1.T / batch_size\n",
    "        \n",
    "        vw2 = beta1 * vw2 + (1 - beta1) * dw2\n",
    "        vw2_hat = vw2/(1-beta1**t)\n",
    "        \n",
    "        sw2 = beta2 * sw2 +(1-beta2) * (dw2 ** 2)\n",
    "        sw2_hat = sw2/(1-beta2**t) \n",
    "        \n",
    "        w2 = w2 - ((learning_rate / (1 + decay * t)) / (np.sqrt(sw2_hat)+epsilon))*(beta1 * vw2_hat+((1-beta1)/(1-beta1**t))*dw2) - lambda_reg * w2\n",
    "\n",
    "        \n",
    "        fprim_net1 = activation_function_derivative(net1)\n",
    "        # dw1 = dE/z*dz/do2*do2/dnet2*dnet2/do1*do1/dnet1*dnet1/dw1 = (z - target) * fprim_net2 * w2 * fprim_net1 * input_data\n",
    "        dw1 = fprim_net1 * (w2_old.T @ (z - batch_labels.T)) @ batch_data.T / batch_size\n",
    "        \n",
    "        vw1 = beta1 * vw1 + (1 - beta1) * dw1\n",
    "        vw1_hat = vw1/(1-beta1**t)\n",
    "        \n",
    "        sw1 = beta2 * sw1 +(1-beta2) * (dw1 ** 2)\n",
    "        sw1_hat = sw1/(1-beta2**t)\n",
    "        \n",
    "        w1 = w1 - ((learning_rate / (1 + decay * t)) / (np.sqrt(sw1_hat)+epsilon))*(beta1 * vw1_hat+((1-beta1)/(1-beta1**t))*dw1) - lambda_reg * w1\n",
    "\n",
    "        error_data_train[i:i+batch_size] = output_layer_error.mean(axis=0)\n",
    "\n",
    "    return {\"w1\": w1, \"w2\": w2, \"output_data_train\": output_data_train, \"error_data_train\": error_data_train}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09364d89",
   "metadata": {},
   "source": [
    "# Local Train Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1386dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_local_model(initials_parameters): \n",
    "    \n",
    "    weights = initials_parameters[\"weights\"]\n",
    "    mse_train = np.zeros((client_epochs, len(optimization_params))) \n",
    "    output_data_trains = np.zeros((len(optimization_params), num_train))\n",
    "    train_accuracy = np.zeros(len(optimization_params))\n",
    "    np.random.seed(42) # for same suffled_indices for all experiments\n",
    "    \n",
    "    for t in range(client_epochs):\n",
    "\n",
    "        shuffled_indices = np.random.permutation(num_train)\n",
    "        data_shuffled = data[:num_train,:num_col - 1][shuffled_indices]\n",
    "        y_one_hot_shuffled = y_one_hot_train[shuffled_indices]\n",
    "        \n",
    "        for index in range(len(optimization_params)):\n",
    "            if(optimizer == 'sgd'):\n",
    "                result = train_with_sgd(data_shuffled,\n",
    "                                        y_one_hot_shuffled,\n",
    "                                        optimization_params[index][\"learning_rate\"],\n",
    "                                        optimization_params[index][\"decay\"],\n",
    "                                        t,\n",
    "                                        weights[index][0],\n",
    "                                        weights[index][1]) # {w1,w2,output_data_train,error_data_train}\n",
    "            elif(optimizer == 'nesterov_momentum'):\n",
    "                result = train_with_nesterov_momentum(\n",
    "                                        data_shuffled,\n",
    "                                        y_one_hot_shuffled,\n",
    "                                        optimization_params[index][\"learning_rate\"],\n",
    "                                        optimization_params[index][\"decay\"],\n",
    "                                        t,\n",
    "                                        optimization_params[index][\"momentum\"],\n",
    "                                        weights[index][0],\n",
    "                                        weights[index][1]) # {w1,w2,output_data_train,error_data_train}\n",
    "            elif(optimizer == 'adagrad'):\n",
    "                result = train_with_adagrad(\n",
    "                                        data_shuffled,\n",
    "                                        y_one_hot_shuffled,\n",
    "                                        optimization_params[index][\"learning_rate\"],\n",
    "                                        optimization_params[index][\"decay\"],\n",
    "                                        t,\n",
    "                                        optimization_params[index][\"epsilon\"],\n",
    "                                        weights[index][0],\n",
    "                                        weights[index][1]) # {w1,w2,output_data_train,error_data_train}\n",
    "            elif(optimizer == 'rmsprop'):\n",
    "                result = train_with_rmsprop(\n",
    "                                        data_shuffled,\n",
    "                                        y_one_hot_shuffled,\n",
    "                                        optimization_params[index][\"learning_rate\"],\n",
    "                                        optimization_params[index][\"decay\"],\n",
    "                                        t, \n",
    "                                        optimization_params[index][\"epsilon\"],\n",
    "                                        optimization_params[index][\"rho\"],\n",
    "                                        weights[index][0], weights[index][1]) # {w1,w2,output_data_train,error_data_train}\n",
    "            elif(optimizer == 'adam'):\n",
    "                result = train_with_adam(\n",
    "                                        data_shuffled,\n",
    "                                        y_one_hot_shuffled,\n",
    "                                        optimization_params[index][\"learning_rate\"],\n",
    "                                        optimization_params[index][\"decay\"],\n",
    "                                        t+1,\n",
    "                                        optimization_params[index][\"epsilon\"],\n",
    "                                        optimization_params[index][\"beta1\"],\n",
    "                                        optimization_params[index][\"beta2\"],\n",
    "                                        weights[index][0],\n",
    "                                        weights[index][1]) # {w1,w2,output_data_train,error_data_train}\n",
    "            \n",
    "            weights[index] = [result[\"w1\"],result[\"w2\"]]\n",
    "            output_data_trains[index] = result[\"output_data_train\"]\n",
    "            mse_train[t][index] = np.mean(result[\"error_data_train\"] ** 2)\n",
    "\n",
    "        \n",
    "        # Print training information\n",
    "        print(f'Epoch: {t + 1}, Train MSE: {mse_train[t,:]}')\n",
    "\n",
    "   \n",
    "        for index in range(len(optimization_params)):\n",
    "        \n",
    "            print('***************************************************************************')\n",
    "            print('optimization_params: ', optimization_params[index][\"description\"])\n",
    "            \n",
    "            # Plotting the training output\n",
    "            plt.figure(figsize=(20, 8))\n",
    "            plt.plot(data[:num_train, num_col-1][shuffled_indices], '-sr')  # Use shuffled indices for labels\n",
    "            plt.plot(output_data_trains[index], '-*b')\n",
    "            plt.xlabel('Train Data')\n",
    "            plt.ylabel('Output')\n",
    "            plt.title('Training Output')\n",
    "            plt.show() \n",
    "\n",
    "            # plotting confusion matrix\n",
    "            plot_confusion_matrix(output_data_trains[index], data[:num_train, num_col - 1][shuffled_indices], \"Train Confusion Matrix\")\n",
    "\n",
    "            # Train accuracy\n",
    "            train_accuracy[index] = np.mean(output_data_trains[index] == data[:num_train, num_col-1][shuffled_indices])\n",
    "            print(f\"Accuracy on the train set: {train_accuracy[index] * 100}%\")  \n",
    "\n",
    "    return {\"client_id\": \"C8\", \"weights\":weights, \"train_mse\":mse_train[-1,:], \"train_accuracy\": train_accuracy, \"num_samples\": num_data}\n",
    "                                                                                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6986a6e4",
   "metadata": {},
   "source": [
    "# Test Global Model(Aggregated Model) On Client Local Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94da9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_aggregated_model(aggregated_parameters):   \n",
    "    \n",
    "    current_server_round = aggregated_parameters[\"current_server_round\"]\n",
    "    \n",
    "    error_data_test = np.zeros((len(optimization_params), num_test))\n",
    "    output_data_test = np.zeros((len(optimization_params), num_test))\n",
    "    test_accuracy = np.zeros(len(optimization_params))\n",
    "    \n",
    "    for index in range(len(optimization_params)):\n",
    "        for i in range(num_test):\n",
    "            input_data = data[num_train + i, :num_col-1].reshape(-1,1) \n",
    "            w1 = aggregated_parameters['weights'][index][0]\n",
    "            w2 = aggregated_parameters['weights'][index][1]\n",
    "            net1 = w1 @ input_data \n",
    "            o1 = activation_function(net1)\n",
    "            net2 = w2 @ o1 \n",
    "            o2 = net2\n",
    "            z = softmax(o2) \n",
    "            output_data_test[index][i] = np.argmax(z)\n",
    "            \n",
    "            error = -(y_one_hot_test[i:i+1] @ np.log10(z,where=z>0)).reshape(-1,1).flatten() \n",
    "            error_data_test[index][i] = error[0]\n",
    "     \n",
    "\n",
    "        mse_test[current_server_round][index] = np.mean(error_data_test[index] ** 2)\n",
    "        \n",
    "        print('optimization_params: ', optimization_params[index][\"description\"])\n",
    "        \n",
    "        # Plotting the test output\n",
    "        plt.figure(figsize=(20, 8))\n",
    "        plt.subplot(2, 2, 1)    \n",
    "        plt.plot(data[num_train:, num_col-1], '-sr')\n",
    "        plt.plot(output_data_test[index], '-*b')\n",
    "        plt.xlabel('Test Data')\n",
    "        plt.ylabel('Output')\n",
    "\n",
    "        # Plotting the test MSE\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.semilogy(np.arange(1, current_server_round + 1), mse_test[:current_server_round,index])\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('MSE Test')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        plot_confusion_matrix(output_data_test[index], data[num_train:, num_col - 1], \"Test Confusion Matrix\")\n",
    "\n",
    "        print('current_server_round: {} \\t'.format(current_server_round+1))\n",
    "        print('MSE_Test: ' ,mse_test[current_server_round][index])\n",
    "\n",
    "\n",
    "        test_accuracy[index] = np.mean(output_data_test[index] == data[num_train:,num_col-1]) \n",
    "        print(f\"Accuracy on the test set: {test_accuracy[index] * 100}%\") \n",
    "    \n",
    "    return {\"client_id\":\"C8\" ,\"test_mse\": mse_test[current_server_round],'test_accuracy':test_accuracy, \"num_samples\": num_data}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eb2126",
   "metadata": {},
   "source": [
    "# Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954b753",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predicted_classes, actual_classes, title): \n",
    "    \n",
    "    # Create a confusion matrix-like matrix\n",
    "    confusion_matrix = np.zeros((num_class, num_class))\n",
    "\n",
    "    # Fill the confusion matrix\n",
    "    for actual, predicted in zip(actual_classes, predicted_classes):\n",
    "        confusion_matrix[actual.astype(int)][predicted.astype(int)] += 1 \n",
    "\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure()\n",
    "    plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Annotate the plot with numbers\n",
    "    for i in range(num_class):\n",
    "        for j in range(num_class):\n",
    "            plt.text(j, i, str(int(confusion_matrix[i, j])), fontsize=12, ha='center', va='center')  # Corrected\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xticks(np.arange(num_class), np.arange(0, num_class))\n",
    "    plt.yticks(np.arange(num_class), np.arange(0, num_class))\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0e5ed",
   "metadata": {},
   "source": [
    "# Client-Server Communication Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2349f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True: \n",
    "    # get server signal => ready: start new learning process on client local data |  terminate: federated-learning process terminate by server\n",
    "    signal = pickle.loads(client_socket.recv(4096))\n",
    "    print('Server Signal: ', signal)\n",
    "    \n",
    "    if 'terminate' in signal:\n",
    "        print(\"Received termination signal. Terminating client.\")\n",
    "        break \n",
    "\n",
    "    print('Start Local Training')\n",
    "\n",
    "    # start train model with initial parameters already recieved from server on local Data\n",
    "    data_to_send = train_local_model(initial_parameters)\n",
    "    data_to_send['num_samples'] = num_data\n",
    "    \n",
    "    print('End Local Training')\n",
    "    \n",
    "    # send trained parameters back to server to aggregate\n",
    "    send_all(client_socket, data_to_send)\n",
    "\n",
    "    print('Sent Updated Params To Server')\n",
    "    \n",
    "    # receive aggregated model from server\n",
    "    aggregated_parameters_size = int.from_bytes(client_socket.recv(4), 'big')  # Receive data size first\n",
    "    aggregated_parameters_data = receive_all(client_socket, aggregated_parameters_size)\n",
    "    aggregated_parameters = pickle.loads(aggregated_parameters_data)\n",
    "    \n",
    "    print('Received Aggregated Model From Server')\n",
    "    \n",
    "    print('Start Evaluating Aggregated Model With Local Test Data') \n",
    "        \n",
    "    # evaluate aggregated model with test-data\n",
    "    test_result = evaluate_aggregated_model(aggregated_parameters)\n",
    "    \n",
    "    print('End Evaluating Aggregated Model With Local Test Data')\n",
    "    \n",
    "    send_all(client_socket, test_result)\n",
    "\n",
    "    print('Sent Evaluated Aggregated Model With Local Test Data Results To Server')\n",
    "    \n",
    "    # set updated-params to inital-params of next federated-learning local training process on client  \n",
    "    initial_parameters = aggregated_parameters\n",
    "\n",
    "    print(\"\\n\\033[1;m\" + \"*\" * 125)   \n",
    "    \n",
    "# Close connection\n",
    "client_socket.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
